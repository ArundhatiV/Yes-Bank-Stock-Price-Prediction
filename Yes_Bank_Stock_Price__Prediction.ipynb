{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArundhatiV/Yes-Bank-Stock-Price-Prediction/blob/main/Yes_Bank_Stock_Price__Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " <hr>\n",
        "\n",
        " #  <center> <font face=\"Lato\" color='red'> ✴ Project Name\n",
        " # <center> 📈<font face=\"Times New Roman\" color=#00518F> Yes Bank Stock Closing Price Prediction. 📉\n",
        "\n",
        " <hr>\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://github.com/ArundhatiV/Yes-Bank-Stock-Price-Prediction/blob/main/Project_Images/Yes_Bank_logo.png?raw=true\" alt=\"Yes Bank Logo\" width=\"800\" height=\"300\"></center>"
      ],
      "metadata": {
        "id": "O3-uGT8wNp73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "\n",
        "##⚙ **Project Type** - Regression\n",
        "##⚙ **Contribution** - Individual\n",
        "##⚙ **Contributor** - Arundhati Varude\n",
        "\n",
        "<hr>"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### This project aims to predict closing prices of stock using regression models. The following points explains the summary of this regression project.\n",
        "\n",
        ">* <b>Exploratory Data Analysis (EDA):</b> In this step, the data was visualized and analysed to gain insights into the data. This step involved data cleaning and data pre-processing. The data was checked for missing values, duplicate records, and outliers. Data visualization techniques were used to identify patterns, trends, and relationships in the data.\n",
        "\n",
        ">* <b>Scrubing:</b> After the EDA, the data was cleaned up by removing missing values, duplicates, and outliers. The data was standardized, and features were normalized.\n",
        "\n",
        ">* <b>Feature Engineering:</b> In this step, new features were created from the existing data to improve the accuracy of the models. Features such as Year, Month, Quarter where created.\n",
        "\n",
        ">* <b>Pre-processing:</b> In this step, the data was prepared for model implementation. The data was split into training and testing datasets. The training dataset was used to train the model, while the testing dataset was used to evaluate the performance of the model.\n",
        "\n",
        ">* <b>Model implementation:</b> In this step, regression models were implemented to predict stock prices. Various regression models such as Linear Regression, Random Forest Regression, and Gradient Boosting Regression were trained on the dataset. The performance of each model was evaluated using metrics such as mean squared error (MSE), mean absolute error (MAE), and R-squared, also performed the cross-validation so that we can assess the performance of model when expose to unseen data. After considering all the indicator and matric we have chosen the Optimal_RandomForest model as final model to implement in production.\n",
        "\n",
        ">* <b>Model explainability:</b> : In this step, the models were analysed to understand the factors that influence the prediction of stock prices using the SHAP explain ability tool. The feature importance was analysed, and the models were interpreted to explain the relationship between the features and the predicted values.\n",
        "\n",
        "#### Overall, this project aimed to predict stock prices using regression models. The project involved data cleaning, feature engineering, pre-processing, model implementation, and model explain ability. The results showed that the implemented `Optimal_RandomForest` models performed well in predicting stock prices, and the project achieved its objectives.\n",
        "<hr>\n",
        "\n",
        "#### <b>Conclusion :- </b>\n",
        "\n",
        "##### 1.The yes bank stock price dataset does not contain any null/missing value, also it's free form outliers.\n",
        "\n",
        "##### 2.While doing data visualization and cleaning we came to the following conclusion:\n",
        ">* Some of the features are right skewed so we have performed the log transformation.\n",
        ">* The dependent variable having string linear correlation with all independent variables.\n",
        ">* There is high multicollinearity present in the data, so we introduce some new features.\n",
        ">* There is sudden drop in the value of stock after 2018.\n",
        ">*  We have seen the sudden increase in the price of stock in 2014 in a window of 10 months.\n",
        ">* The VIFs values are extremely large, so we drop some features to reduce the VIFs scores.\n",
        ">* We used the StandardScaler to scale our features.\n",
        "\n",
        "##### 3. In the hypothesis testing we mostly find the status quo, and one important conclusion is that the impact of COVID-19 is less as compared to the scam in 2020.\n",
        "\n",
        "##### 4. We have implemented the following regression models, so that we can assure that we get the best fit model.\n",
        ">* Linear Regression\n",
        ">* Ridge Regression\n",
        ">* Lasso Regression\n",
        ">* Polynomial Fit\n",
        ">* Random Forest Regressor\n",
        ">* eXtreme Gradient Boosting (XGBoost) Regressor.\n",
        "\n",
        "##### 5.Upon implementing the given regression models, we came to the following important conclusions:\n",
        ">* The Lasso performed better when compared to linear, ridge, polyfit.\n",
        ">* Polynomial fit performed bad in cross-validation as it shows the sign of overfitting.\n",
        ">* The RandomForest and XGBoost are the final nominees for the model selection. as their overall performance in all matrices and in cross-validation are good compared to others.\n",
        ">* Finally, we choose the RandomForest regressor, taking into consideration time-complexity and cross-validation score (mean CV = 95.77).\n",
        ">* After model selection we used SHAP score to explain the model by using various plots and came to know that OHL feature dominates the other feature as its correlation with DV is high and the second most important feature is Year.\n",
        ">* At the end of the project we save our selected model in joblib and perform some sanity checks."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <center>![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)</center>\n",
        "\n",
        "## 🔗https://github.com/ArundhatiV/Yes-Bank-Stock-Price-Prediction/blob/main/Stock_Price__Prediction.ipynb"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 🏁 [Yes Bank](https://en.wikipedia.org/wiki/Yes_Bank) is a well-known bank in the Indian financial domain. Since 2018, it has been in the news because of the <u>[fraud case involving Rana Kapoor.](https://blog.ipleaders.in/analysis-yes-bank-crisis) Owing to this fact, it was interesting to see how that impacted the stock prices of the company and whether Time series models or any other predictive models can do justice to such situations.</u> This dataset has monthly stock prices of the bank since its inception and includes closing, starting, highest, and lowest stock prices of every month. <u>The main objective is to predict the stock’s closing price of the month.</u>\n",
        "\n",
        "##### 🏁 A bank plays an important role in maintaining the economical condition of the country. The Yes Bank crisis created panic among the people in March 2020 as it was the biggest private sector bank. The failure of a bank, be it a public sector bank or a private sector bank, can affect everyone. In March 2020, news came out that there is a high chance of a Yes bank collapse which caused panic among the depositors. The Reserve Bank of India decided to solve this issue. Reserve Bank of India superseded Yes Bank board of directors for a period of 30 days."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🚦Framing The Right Questions For The Analysis\n",
        "<hr>\n",
        "\n",
        "##### As stated in the general problem statement and overall purpose of the project, 'create the regression model that can predict the closing price of the stock'. Before we get into the specifics of creating a machine learning model, it is crucial to define the questionaries. We therefore come up with the following set of questions after performing the initial inspection of the dataset.\n",
        "\n",
        "\n",
        ">1. Are there any outliers or anomalies in the data that need to be addressed?\n",
        "\n",
        ">2. How are the data points distributed across time?\n",
        "\n",
        ">3. What is the distribution of the stock prices for each feature (Open, High, Low, Close)?\n",
        "\n",
        ">4. Are there any unusual spikes or dips in the data that need to be investigated further?\n",
        "\n",
        ">5. Can we identify any potential outliers or anomalies in the data?\n",
        "\n",
        ">6. Is closing price of ${n}^{th}$ day impacts the opening price of ${(n+1)}^{th}$ day?\n",
        "\n",
        ">7. Are there any significant differences between the different features of the dataset?\n",
        "\n",
        ">8. What additional insights can be gained by visualizing the data in different ways (e.g., scatter plots, box plots, heat maps, etc.)?\n",
        "\n",
        ">9. What is the overall trend of the stock prices over time?\n",
        "\n",
        ">10. Are there any seasonal patterns or trends in the data that can be observed?\n",
        "\n",
        ">11. Which features have the strongest correlation with the closing price of the stock?\n",
        "\n",
        ">12. How accurately can we predict the future closing prices of the stock based on past data?\n",
        "\n",
        ">13. Can the model be improved by adding additional features or applying different regression techniques?\n",
        "\n",
        ">14. What is the most optimal way to train and test the model?\n",
        "\n",
        ">15. How can the model be interpreted and applied in real-world scenarios?"
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ◼ How It Works : A Simple Guide On Stock Exchange Operation.\n",
        "<hr>\n",
        "\n",
        ">♦ The stock market is a platform where publicly traded companies issue and sell shares to raise capital.\n",
        "\n",
        ">♦ Investors buy and sell these shares on exchanges such as BSE, NSE etc.\n",
        "\n",
        ">♦ The price of a share is determined by supply and demand in the market. If more people want to buy a particular stock, its price will go up. Conversely, if more people want to sell a stock, its price will go down.\n",
        "\n",
        ">♦ Investors can place buy or sell orders through a broker or an online trading platform. There are two types of orders: market orders and limit orders. A market order is executed immediately at the current market price, while a limit order is executed only if the price reaches a specified limit.\n",
        "\n",
        ">♦ The stock market is open for trading on weekdays, usually from 9:15 am to 3:30 pm IST. However, some exchanges may have different hours of operation.\n",
        "\n",
        ">♦ The stock market is affected by various factors such as economic indicators, company earnings reports, geopolitical events, and investor sentiment. These factores may affect the volatiality of any stock.\n",
        "\n",
        ">♦ The stock market provides investors with opportunities to earn profits through capital gains or dividends, but it also involves risks such as the potential for loss of capital.\n",
        "\n",
        ">♦ The stock market plays a crucial role in the economy, as it provides companies with a means to raise capital for growth and expansion, and provides investors with an opportunity to participate in the growth of these companies.\n",
        "\n",
        ">♦ Stock markts also help in price discovery and real value of the companies subjected to market conditions."
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ◼ Business Impact Analysis.\n",
        "<hr>\n",
        "\n",
        "##### An agency whether it is Individual or compnay that deals with trading or investing in Yes Bank stocks may be significantly impacted by this regression project. These would be a few potential impacts on business:\n",
        "\n",
        "><b>♦ Smarter investment choices:</b> By accurately predicting the closing price of Yes Bank stocks, investors can make better-informed and data driven investment decisions. This can lead to increased profits and a more efficient use of capital.\n",
        "\n",
        "><b>♦ Risk management:</b> Predicting the closing price of Yes Bank stocks can also help with risk management. Investors can use the predictions to identify potential risks and take steps to mitigate them. For example, if the model predicts a significant drop in stock prices, investors can take steps to reduce their exposure to Yes Bank stocks.\n",
        "\n",
        "><b>♦ Increased competitiveness:</b> Businesses that are able to accurately predict the closing price of Yes Bank stocks may have a competitive advantage over their competitors. This is because they can make better investment decisions and manage risks more effectively.\n",
        "\n",
        "><b>♦ Improved customer satisfaction:</b> For businesses that offer investment advice or manage portfolios on behalf of clients, accurate stock price predictions can lead to improved customer satisfaction. This is because clients are more likely to be satisfied with their investment returns if their investments are based on accurate predictions."
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "\n",
        "# <center> <font face=\"Lato\" color='red'>✴ General Guidelines\n",
        "\n",
        "<hr>\n",
        "\n",
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "9lHknwjxPzg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://github.com/ArundhatiV/Yes-Bank-Stock-Price-Prediction/blob/main/Project_Images/Key_stages_in_ds_project.png?raw=true\" alt=\"Data Science Project Stages\" width=\"800\" height=\"350\">"
      ],
      "metadata": {
        "id": "Hb2ofdpQkACj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### ♦ <u>Obtain</u>: In this stage, the data scientist identifies the data required to solve the problem at hand and gathers it from various sources. This can involve collecting data from databases, web scraping, API calls, or data purchases. For this project we already given the data in CSV format.\n",
        "\n",
        "##### ♦ <u>Scrub</u>: In this stage, the data is cleaned, formatted, and prepared for analysis. This can involve removing missing values, dealing with outliers, handling data inconsistencies, and standardizing the format of the data. The goal is to ensure that the data is reliable and usable for analysis.\n",
        "\n",
        "##### ♦ <u>Explore</u>: In this stage, the data scientist performs exploratory data analysis (EDA) to understand the characteristics of the data and identify any patterns or trends. This can involve creating visualizations, calculating summary statistics, and identifying correlations between variables.\n",
        "\n",
        "##### ♦ <u>Model</u>: In this stage, the data scientist builds predictive models using the data. This can involve selecting appropriate algorithms, training and testing models, and tuning hyperparameters. The goal is to create a model that accurately predicts the target variable.\n",
        "\n",
        "##### ♦ <u>Interpret</u>: In this stage, the data scientist interprets the results of the modeling process and draws conclusions from them. This can involve identifying which features are most important, evaluating the accuracy of the model, and determining whether the model can be used to make decisions in the real world. The goal is to provide insights that can be used to solve the problem at hand.\n",
        "\n"
      ],
      "metadata": {
        "id": "G-VPCUaGkz7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://github.com/ArundhatiV/Yes-Bank-Stock-Price-Prediction/blob/main/Project_Images/Project_Architecture.png?raw=true\" alt=\"Data Science Project Stages\" width='1080' height=\"720\">"
      ],
      "metadata": {
        "id": "rS0Rr2VKlBzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***\n",
        "\n",
        "### Dataset Description:\n",
        "The dataset contains monthly stock prices with the following columns:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Date**: Date of the stock price record\n",
        "\n",
        "**Open**: Opening price of the stock\n",
        "\n",
        "**High**: Highest price of the stock for the month\n",
        "\n",
        "**Low**: Lowest price of the stock for the month\n",
        "\n",
        "**Close**: Closing price of the stock for the month"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Numpy for computationally effcient operations\n",
        "import numpy as np\n",
        "\n",
        "# Pandas for data manipulation and aggregation\n",
        "import pandas as pd\n",
        "\n",
        "# Matplotlib and seaborn for visualisation and behaviour wrt target variable\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# For Candlestick chart\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from numpy import math\n",
        "from datetime import datetime\n",
        "\n",
        "# Scipy distribution for Hypothesis Testing\n",
        "from scipy.stats import uniform\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import chi2\n",
        "from scipy.stats import t\n",
        "from scipy.stats import f\n",
        "from scipy.stats import ttest_rel, ttest_ind\n",
        "\n",
        "# Variance Inflation Factor\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Scikit Learn for medel training, model optimization, and metrics calculation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Regularization Model\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "# Polynomial fit\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Scikit Learn metrics to measure the performance of model\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "\n",
        "\n",
        "#Cros-Validation and HyperParameter tunning\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Random Forest Regression Model\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Install and import scikit-plot\n",
        "import subprocess\n",
        "batcmd = \"pip install scikit-plot==0.3.7\"\n",
        "result = subprocess.check_output(batcmd, shell=True)\n",
        "\n",
        "# Importing scikitplot\n",
        "import scikitplot as skplt\n",
        "\n",
        "# XGBoost Regressor Model\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Installing SHAP for Model Explainability\n",
        "batcmd = \"pip install shap\"\n",
        "result = subprocess.check_output(batcmd, shell=True)\n",
        "\n",
        "# Importing shap\n",
        "import shap\n",
        "\n",
        "# Saving the model in joblib\n",
        "import joblib\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "# Fetching the dataset url from GitHub\n",
        "url ='https://raw.githubusercontent.com/ArundhatiV/Yes-Bank-Stock-Price-Prediction/main/Data%20Files/data_YesBank_StockPrices%20(1).csv'\n",
        "\n",
        "# Creating the Pandas DataFrame with the given row data\n",
        "dataset = pd.read_csv(url, encoding= 'unicode_escape')\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the copy of the dataset\n",
        "df = dataset.copy()"
      ],
      "metadata": {
        "id": "ZwWicO1unUlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* It is important to create the `copy` of the data, as further in the project we do various operations on the dataset such as `Data wrangling`, `Data Transformation`, `Feature Engineering` or `Feature Transformation`. upon doing such operations we get different dataset at the end depending on our requirement at that time so it is always a good idea to keep the orginal row data intact so we can always revert back the chnages we did in the overall process."
      ],
      "metadata": {
        "id": "DOwO3c63nhC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Middle five observations\n",
        "# Let's create the python function which retuen the records in the middle of the dataframe works same as default head() method\n",
        "def get_middle_rows(df, num_rows=5):\n",
        "\n",
        "    # doc string\n",
        "    \"\"\"\n",
        "    This function takes a Pandas DataFrame as input and returns a specified number of rows from the middle of the DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "        df (pandas.DataFrame): Input DataFrame.\n",
        "        num_rows (int, optional): Number of rows to return from the middle of the DataFrame. Default is 5.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: Specified number of rows from the middle of the input DataFrame.\n",
        "    \"\"\"\n",
        "    # getting the middle of the DataFrame\n",
        "    middle_index = len(df) // 2\n",
        "\n",
        "    # calculating the starting and ending index\n",
        "    start_index = middle_index - (num_rows // 2)\n",
        "    end_index = start_index + num_rows\n",
        "\n",
        "    # return the DataFrame\n",
        "    return df.iloc[start_index:end_index]"
      ],
      "metadata": {
        "id": "f_ejnHcaoPWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_middle_rows(df)"
      ],
      "metadata": {
        "id": "560egEdboT3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Last five observations\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "NEs1g0mXoXZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Upon doing the early inspection of the `Yes Bank Stock Closing Price` dataset we can conclude that, The `185 observations` are segmented into `5 features/variables`."
      ],
      "metadata": {
        "id": "sDlfuMUeocJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "# Dataset Rows & Columns count\n",
        "print(f'The number of rows in the dataset is: ', df.shape[0])\n",
        "\n",
        "print(f'The number of columns in the dataset is: ', df.shape[1])"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yg0-_t0qoddy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* There are multiple features in the dataset - Date, Open, High and close The columns Open and Close represent the starting and final price.\n",
        "\n",
        ">* The High, Low features are represent the maximum and minimum."
      ],
      "metadata": {
        "id": "Cb1xRDf1o9Bn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "# Let's create the python function to determine the duplicated count based on all vs individual\n",
        "def count_duplicates(df):\n",
        "\n",
        "    # doc string\n",
        "    \"\"\"\n",
        "    This function takes a Pandas DataFrame as input and returns the number of duplicated rows based on all columns,\n",
        "    as well as the number of duplicated values for each column.\n",
        "\n",
        "    Parameters:\n",
        "        df (pandas.DataFrame): Input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - int: Number of duplicated rows based on all columns.\n",
        "            - pandas.Series: Number of duplicated values for each column.\n",
        "    \"\"\"\n",
        "    # calculate the duplicated rows based on all columns\n",
        "    duplicates_all_cols = df.duplicated().sum()\n",
        "\n",
        "    # calculate the duplicated rows based on individual column\n",
        "    duplicates_per_col = df.shape[0] - df.nunique()\n",
        "\n",
        "    return duplicates_all_cols, duplicates_per_col"
      ],
      "metadata": {
        "id": "usJjasy-pDkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function to get the count of duplicated row\n",
        "duplicates_all_cols, duplicates_per_col = count_duplicates(df)"
      ],
      "metadata": {
        "id": "jnsiclFHpRn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results\n",
        "print(\"Number of duplicate rows (based on all columns):\", duplicates_all_cols)\n",
        "print(\"Number of duplicate values per column:\")\n",
        "print(duplicates_per_col)"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.displot(\n",
        "    data=df.isna().melt(value_name=\"missing\"),\n",
        "    y=\"variable\",\n",
        "    hue=\"missing\",\n",
        "    multiple=\"fill\",\n",
        "    aspect=1\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's create a custom function to create a dataframe which contain the metadata of the dataset\n",
        "# define a function.\n",
        "def metainfo(given_df):\n",
        "\n",
        "  # doc string contain a short description about the function useability.\n",
        "  '''This function return the new dataframe called meta_structure, & gives metadata about dataframe.\n",
        "\n",
        "  Parameters:\n",
        "        df (pandas.DataFrame): Input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame with customized features.\n",
        "\n",
        "  '''\n",
        "\n",
        "  # we want each attribute of the dataframe as a row in new datafrme.\n",
        "  meta_structure = pd.DataFrame(index=given_df.columns)\n",
        "\n",
        "  # let's create a six new column which defines our metadata.\n",
        "  meta_structure[\"data_type\"] = given_df.dtypes\n",
        "  meta_structure[\"not_null_values\"] = given_df.count()\n",
        "  meta_structure[\"null_values\"] = given_df.isnull().sum()\n",
        "  meta_structure[\"null_values_percentage\"] = round(given_df.isnull().mean(),5) * 100\n",
        "  meta_structure[\"unique_count\"] = given_df.nunique()\n",
        "  meta_structure[\"duplicated_count\"] = given_df.shape[0] - given_df.nunique()\n",
        "\n",
        "  # just return the created structure.\n",
        "  return meta_structure"
      ],
      "metadata": {
        "id": "iEeBwEeQpsw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function calling.\n",
        "metainfo_df = metainfo(df)\n",
        "\n",
        "# let's print the meta_df\n",
        "metainfo_df"
      ],
      "metadata": {
        "id": "s3nje0SVpy1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### From the Initial inspection of the dataset we gather the following points :\n",
        "\n",
        ">1. The Dataset having 185 observations.\n",
        "\n",
        ">2. These 185 observation has 5 features those features are :\n",
        "\n",
        ">><b>Date</b><br>\n",
        ">><b>Open</b><br>\n",
        ">><b>High</b><br>\n",
        ">><b>Low</b><br>\n",
        ">><b>Close</b><br>\n",
        "\n",
        ">3.   One single observation tell us how the stock of the Yes Bank performed over the month.\n",
        ">4.   The data set don't have Null/Missing values.\n",
        ">5.   The data set also don't contain the duplicated records."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df_features = list(df.columns)\n",
        "\n",
        "df_features"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description\n",
        "##### The `Yes Bank Stock Closing Price`dataset consist of the following 5 features\n",
        "\n",
        "> <b>Date: </b> The Date feature is the collection of date values, when the values of attributes are observed.\n",
        "\n",
        "> <b>Open:</b> The opening price is the price at which a stock first trades upon the opening of an exchange on a trading day.\n",
        "\n",
        "> <b>High:</b> It is the highest price of the stock in the treading day.\n",
        "\n",
        "> <b>Low:</b> It is the lowest price of the stock in the teading day.\n",
        "\n",
        "> <b>Close:</b> The closing price is a stock's trading price at the end of a trading day. The closing price is calculated as the weighted average price of the last 30 minutes, i.e. from 3:00 PM to 3:30 PM in case of equity.\n",
        "\n",
        "\n",
        "  \\begin{array}{|c|c|c|c|c|c|c|c|}\\hline\\\\ \\\\\n",
        "  \\mathcal{} & \\mathcal{Date} & \\mathcal{Open} & \\mathcal{High} & \\mathcal{Low} & \\mathcal{Close/(Dependable Variable)} \\\\ \\hline\\\\ \\\\\n",
        "  1  \\\\ \\hline\\\\ \\\\\n",
        "  2 \\\\ \\hline\\\\ \\\\\n",
        "  3 \\\\ \\hline\\\\ \\\\\n",
        "  \\end{array}"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <b>The Flavors Of Data And The Scales Of Data Measurement</b>\n",
        "\n",
        ">* Before, we start our analysis it better to discuss the flavors of data and it's scales of measurement. It is important to discuss this topic because for each type of scale the methodology for statistical analysis will be changed.\n",
        "\n",
        "##### <b>Flavors of data</b>\n",
        "<br>\n",
        "<center><img src=\"https://github.com/ashish-mali/Regression-Yes-Bank-Stock-Closing-Price-Prediction/blob/main/Images/type_of_data.jpg?raw=true\" alt=\"Yes Bank Logo\" width=\"auto\" height=\"auto\"></center>\n",
        "<br>\n",
        "\n",
        ">* It is important to understand the different flavors of data for several reasons. Not only will the type of data dictate the method used to analyze and extract results. knowing whether the data is unstructured or perhaps quantitative can also tell you a lot about the real-world phenomenon being measured. Let's look at the three basic classification of data :\n",
        "\n",
        ">>* Structured vs Unstructured $/$ (Organized vs Unorganized).\n",
        "\n",
        ">>* Quantitative vs Qualitative.\n",
        "\n",
        ">>>* Nominal data.\n",
        ">>>*Ordinal data.\n",
        ">>>*Discrete data.\n",
        ">>>*Continuous data.\n",
        "\n",
        "##### <b>The scales of data measurement</b>\n",
        "<br>\n",
        "<center><img src=\"https://github.com/ashish-mali/Regression-Yes-Bank-Stock-Closing-Price-Prediction/blob/main/Images/level_of_data.jpg?raw=true\" alt=\"Yes Bank Logo\" width=\"800\" height=\"auto\"></center>\n",
        "<br>\n",
        "\n",
        ">* It is generally understood that a specific characteristic (feature / column) of structured data can be broken down into aone of four levels of data. As we move form Nominal to Ratio, we gain more structure and therefore more returns from our analysis. Each level comes with it's own accepted practicve in measuring the conter of the data. We usually think of the mean/average as being an acceptable form of center, howerver, this is only true for a specific type of data.\n",
        "\n",
        ">* Let's define our given feature by usign the above frameworks.\n",
        "\n",
        ">>* <b>Date :-</b> This feature has a  data type of <u>Categorical - Ordinal</u> and scale is <u>Interval level</u>.\n",
        ">>* <b>Open :-</b> This feature has a  data type of <u>Quantitative - Continuous</u> and scale is <u>Ratio level</u>.\n",
        ">>* <b>High :-</b>This feature has a  data type of <u>Quantitative - Continuous</u> and scale is <u>Ratio level</u>.\n",
        ">>* <b>Low :-</b>This feature has a  data type of <u>Quantitative - Continuous</u> and scale is <u>Ratio level</u>.\n",
        ">>* <b>Close :-</b>This feature has a  data type of <u>Quantitative - Continuous</u> and scale is <u>Ratio level</u>."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <b>Q. Why opening price of the stock is not same as closing price?</b>\n",
        "\n",
        ">* As we know that the closing price of the stock is simply the weighted average price of the last 30 minutes, and can be calculated as :\n",
        "<br>\n",
        "\n",
        "#### <center>$Closing Price = {Total Treaded Value}÷{TotalVolume}$\n",
        "<br>\n",
        "\n",
        ">* Also, The opening price is the price at which a stock first trades upon the opening of an exchange on a trading day. For equities, the normal market timing is from 9.15 am to 3.30 pm. But, the exchange starts collecting orders from people at 9.00 am till 9.08 am called as pre-market window, during this time, they collect the orders from the public and during the next 7 minutes before markets open, they match these orders to decide at what price the stock will open for the day at 9:15.\n",
        "\n",
        ">* So as you can see from the above explanation, there's a pre-market window with which the opening price is calculated, where depending upon the demand and supply of a stock, the opening price may be different from its previous day closing price.\n",
        "\n",
        ">* In the hours between the closing price and the next trading day's opening price, several factors can affect the price of a particular stock. Some of the factors are :\n",
        "\n",
        ">> After Market Order (AMO)\n",
        "\n",
        ">>News About a Company."
      ],
      "metadata": {
        "id": "fxuTOeQbqzdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "# Checking the no of unique value in the Date feature\n",
        "no_uq_values_date = df['Date'].nunique()\n",
        "\n",
        "print(f'The total number of unique values present in date feature is:', no_uq_values_date)"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the no of unique value in the open feature\n",
        "no_uq_values_open = df['Open'].nunique()\n",
        "\n",
        "print(f'The total number of unique values present in open feature is:', no_uq_values_open)"
      ],
      "metadata": {
        "id": "a9xWLrq3rJ-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see which values are duplicated\n",
        "df['Open'].value_counts()"
      ],
      "metadata": {
        "id": "yZLHjeFerNoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching the Index corresponding to the value 25.60\n",
        "duplicate_25_60 = np.where(df['Open'] == 25.60)\n",
        "\n",
        "# Fetching the Index corresponding to the value 89.20\n",
        "duplicate_89_20 = np.where(df['Open'] == 89.20)\n",
        "\n",
        "# Print the statement\n",
        "print(f'The Index of duplicated value 25.60:', duplicate_25_60)\n",
        "\n",
        "# Print the statement\n",
        "print(f'The Index of duplicated value 89.20:', duplicate_89_20)"
      ],
      "metadata": {
        "id": "piHlSblnrVqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching the correcsponding record\n",
        "df.iloc[45]"
      ],
      "metadata": {
        "id": "srLCGuMErZwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching the correcsponding record\n",
        "df.iloc[180]"
      ],
      "metadata": {
        "id": "tcDa0moxreUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching the correcsponding record\n",
        "df.iloc[90]"
      ],
      "metadata": {
        "id": "Uqxi63U-rqxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching the correcsponding record\n",
        "df.iloc[169]"
      ],
      "metadata": {
        "id": "1ksT4Np6rwh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>As from the above it's clear that there is no duplicated record and all observation corresponding to the given feature are unique.</b>"
      ],
      "metadata": {
        "id": "JAH40Chhr_BB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the no of unique value in the high feature\n",
        "no_uq_values_high = df['High'].nunique()\n",
        "\n",
        "print(f'The total number of unique values present in high feature is:', no_uq_values_high)"
      ],
      "metadata": {
        "id": "_6UOOwzosHAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see which values are duplicated\n",
        "df['High'].value_counts()"
      ],
      "metadata": {
        "id": "jePjZObZsKzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching the Index corresponding to the value 17.16\n",
        "duplicate_17_16 = np.where(df['High'] == 17.16)\n",
        "\n",
        "# Print statement\n",
        "print(f'The Index of duplicated value 1716:', duplicate_17_16)"
      ],
      "metadata": {
        "id": "_4daZZJ8sMkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching the correcsponding record\n",
        "df.iloc[6]"
      ],
      "metadata": {
        "id": "2rMSQ4B_sVlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching the correcsponding record\n",
        "df.iloc[18]"
      ],
      "metadata": {
        "id": "L8tifubcsVMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the no of unique value in the low feature\n",
        "no_uq_values_low = df['Low'].nunique()\n",
        "\n",
        "print(f'The total number of unique values present in low feature is:', no_uq_values_low)"
      ],
      "metadata": {
        "id": "ERYGSLyLsU7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see which values are duplicated\n",
        "df['Low'].value_counts()"
      ],
      "metadata": {
        "id": "2WVSeaXRsjzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching the Index corresponding to the value 11.25\n",
        "duplicate_11_25 = np.where(df['Low'] == 11.25)\n",
        "\n",
        "# Fetching the Index corresponding to the value 33.60\n",
        "duplicate_33_60 = np.where(df['Low'] == 33.60)\n",
        "\n",
        "# Print statement\n",
        "print(f'The Index of duplicated value 11.25:', duplicate_11_25)\n",
        "\n",
        "# Print statement\n",
        "print(f'The Index of duplicated value 33.60:', duplicate_33_60)"
      ],
      "metadata": {
        "id": "c4wj3mvYsobf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching the correcsponding record\n",
        "df.iloc[0]"
      ],
      "metadata": {
        "id": "BTWUqPrNs3KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching the correcsponding record\n",
        "df.iloc[40]"
      ],
      "metadata": {
        "id": "B0x-Faf2s3Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching the correcsponding record\n",
        "df.iloc[30]"
      ],
      "metadata": {
        "id": "xYrMjMSDs3C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching the correcsponding record\n",
        "df.iloc[175]"
      ],
      "metadata": {
        "id": "6_pzL-yGs2_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>As from the above it's clear that there is no duplicated record and all observation corresponding to the given feature are unique.</b>"
      ],
      "metadata": {
        "id": "wCumjNSntJub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the no of unique value in the Date feature\n",
        "no_uq_values_close = df['Close'].nunique()\n",
        "\n",
        "print(f'The total number of unique values present in close feature is:', no_uq_values_close)"
      ],
      "metadata": {
        "id": "kHfsjXgMs279"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Now, we can conclude that our dataset is free from duplicated values .</b>\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "s7lTa8yitThR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "><b>Data wrangling</b>, sometimes referred to as <b>data munging</b>, is the process of transforming and mapping data from one \"raw\" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics."
      ],
      "metadata": {
        "id": "N8O0elA4tmnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Missing/Null values treatment\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> No null/missing values in the given dataset\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "j0RqC_Wat9gh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate value treatment"
      ],
      "metadata": {
        "id": "zvLzlEoFdNww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking for duplicated values\n",
        "len(df[df.duplicated()])"
      ],
      "metadata": {
        "id": "Y82ZehngdO1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Dataset is free form duplicated values\n"
      ],
      "metadata": {
        "id": "goaMLSPKdZ5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Correcting the data types\n"
      ],
      "metadata": {
        "id": "qBFcK6g0djKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "9s79x43admuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Here we have seen that among all the 5 features only <b>`Date`</b> feature needs the correction. so we have to convert the string data type to datetime format.\n",
        "\n",
        ">* Looking at the observations which `Date` feature holds, having the format of `MM-YY`. So we need to convert it into proper `YYYY-MM-DD` format.\n",
        "\n",
        ">* The other 4 features datatypes are correct."
      ],
      "metadata": {
        "id": "BlQAVl7CdwV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the `Date` feature datatype to proper `YYYY-MM-DD`\n",
        "df['Date'] = df['Date'].apply(lambda x : datetime.strptime(x, '%b-%y'))"
      ],
      "metadata": {
        "id": "Zh2fBwu0d1xI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> As we corrected the data type of `Date` feature, lets extract some other feature to analyze better."
      ],
      "metadata": {
        "id": "VJlxWUjkdxxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract year from date feature\n",
        "df['Year'] = df['Date'].dt.year"
      ],
      "metadata": {
        "id": "ZoaoMy04eAm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the transformation.\n",
        "df.head()"
      ],
      "metadata": {
        "id": "cpyeVT9GeEQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract month from date feature\n",
        "df['Month'] = df['Date'].dt.month"
      ],
      "metadata": {
        "id": "Qe45hXzdeTwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract month from date feature\n",
        "df['Quarter'] = df['Date'].dt.quarter"
      ],
      "metadata": {
        "id": "H8-dHJ2veTtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check unique values for 'Year' feature\n",
        "df['Year'].unique()"
      ],
      "metadata": {
        "id": "1tqt_FzFeTp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check unique values for 'Year' feature\n",
        "df['Month'].unique()"
      ],
      "metadata": {
        "id": "u8H70EOLeTcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check unique values for 'Year' feature\n",
        "df['Quarter'].unique()"
      ],
      "metadata": {
        "id": "pNi7WqPgekAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the reflection\n",
        "df.head()"
      ],
      "metadata": {
        "id": "hJCW7f9vej8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the reflection\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "-573PoSYej4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Now, all the features having the proper datatypes."
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Outlier Analysis"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create the function which create the boxplot for given DataFrame.\n",
        "def plot_box_plots(df):\n",
        "\n",
        "    # Doc. sting\n",
        "    '''\n",
        "    This function creates the boxplot for each numerical features for given DataFrame\n",
        "    '''\n",
        "    # Selectin only numerical features\n",
        "    numerical_columns = df.select_dtypes(include=['float', 'int']).columns\n",
        "\n",
        "    # Define the subplots\n",
        "    fig, ax = plt.subplots(1, len(numerical_columns), figsize=(20, 5))\n",
        "\n",
        "    # looping & boxplot creation\n",
        "    for i, column in enumerate(numerical_columns):\n",
        "        ax[i].boxplot(df[column])\n",
        "        ax[i].set_title(f'Box Plot of {column}', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # show the plots\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "yuOc7WwMe7s1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function calling\n",
        "plot_box_plots(df)"
      ],
      "metadata": {
        "id": "txV_d_loe8XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Seems to have some outliers in the key features of the dataset. Let's invisticate further and find out whether they belong to outlier category or not.\n",
        "\n",
        ">*  Also, form last 3 plots correctly finds the median values of the data."
      ],
      "metadata": {
        "id": "l_k06moofJ6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a function to find outliers using IQR\n",
        "def find_outliers_IQR(df):\n",
        "\n",
        "    #doc. string\n",
        "    '''\n",
        "    This function finds the outlier in the given DataFrame\n",
        "    '''\n",
        "    q1= df.quantile(0.25)\n",
        "    q3= df.quantile(0.75)\n",
        "\n",
        "    #Caluclate the IQR\n",
        "    IQR=q3-q1\n",
        "    outliers = df[((df<(q1-1.5*IQR)) | (df>(q3+1.5*IQR)))]\n",
        "\n",
        "    # return statement\n",
        "    return outliers"
      ],
      "metadata": {
        "id": "UOIawel7fNqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find outliers percentage\n",
        "for i in ['Open' , 'High', 'Low', 'Close']:\n",
        "  outliers = find_outliers_IQR(df[i])\n",
        "  outliers_percent = round((outliers.size/df[i].size)*100, 2)\n",
        "  print(f'{i} : {outliers_percent}%')"
      ],
      "metadata": {
        "id": "U-jCgfpbfSOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* The threshold for identifying outliers should be based on a careful examination of the data and the specific goals of the analysis, rather than a fixed percentage or rule of thumb.\n",
        "\n",
        ">* The appropriate threshold for identifying outliers can depend on a variety of factors, such as the nature of the data, the sample size, and the goals of the analysis. In some cases, a relatively small number of extreme values can have a large impact on the analysis, and therefore should be considered as outliers even if they make up a small percentage of the data. In other cases, a larger percentage of extreme values may be acceptable if they do not significantly affect the analysis.\n",
        "\n",
        ">* Therefore, it is important to carefully examine the data and consider the context of the analysis before determining the appropriate threshold for identifying outliers.\n",
        "\n",
        "> * Let's delve deeper to uncover more insides."
      ],
      "metadata": {
        "id": "VuuIVP55fW1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find outliers\n",
        "for i in ['Open' , 'High', 'Low', 'Close']:\n",
        "  outliers = find_outliers_IQR(df[i])\n",
        "  print(f'{i} : ')\n",
        "  print(outliers)"
      ],
      "metadata": {
        "id": "GIFIIIJbfbSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's fetch the records form 144 to 157 to analyze further."
      ],
      "metadata": {
        "id": "5mgk-tmpfjqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching the records corresponding to required indexes\n",
        "df[144:158]"
      ],
      "metadata": {
        "id": "8jNdLAAtfmt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We calculated the percentage we found that one 'High' column has more than 2.5 percent and all the other 3 columns have almost same which is 4.86 percent outliers.\n",
        "\n",
        ">* We know that these are stock prices, and all the outliers in all 4 columns are almost similar value which is more than 300.\n",
        "\n",
        ">* From all the above points we can say that these are not outliers , these are possible values."
      ],
      "metadata": {
        "id": "mCMwLi5ofrq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Let's do some aggregate operations to better understand the features with respected to newly created features."
      ],
      "metadata": {
        "id": "wlbj9XNyfwti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check min, max and mean prices for each year feature\n",
        "df.groupby('Year').aggregate({'Open' : [min, max, 'mean'], 'High' : [min, max, 'mean'], 'Low' : [min, max, 'mean'], 'Close' : [min, max, 'mean']})"
      ],
      "metadata": {
        "id": "0cp0jhuBf03f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* In the ten year we have seen the Up-Down hill like that is the price in 2005 (at start) is same as price in 2020 (at end).  \n",
        "\n",
        ">* During the this period the prices go as high as $300^+$ and as low as $12^-$.\n",
        "\n",
        ">* The year of high is 2017-18 and low is 2005 and 2020."
      ],
      "metadata": {
        "id": "ePVOw7BTf8jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check min, max and mean prices for each Month feature\n",
        "df.groupby('Month').aggregate({'Open' : [min, max, 'mean'], 'High' : [min, max, 'mean'], 'Low' : [min, max, 'mean'], 'Close' : [min, max, 'mean']})"
      ],
      "metadata": {
        "id": "0IJObkrZf796"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We have observed that the prices are quite low in the $10^{th}$ and $11^{th}$ month compaired to other months.\n",
        "\n"
      ],
      "metadata": {
        "id": "V74oX3ZwgN4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check min, max and mean prices for each Quarter feature\n",
        "df.groupby('Quarter').aggregate({'Open' : [min, max, 'mean'], 'High' : [min, max, 'mean'], 'Low' : [min, max, 'mean'], 'Close' : [min, max, 'mean']})"
      ],
      "metadata": {
        "id": "3HHomXebgWqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* In the $2^{nd}$ quarter, all the prices mean are higher.\n",
        "\n",
        ">* In the $4^{th}$ quarter, all the prices mean are lower."
      ],
      "metadata": {
        "id": "-6sHkk9rgb35"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?\n",
        "\n",
        ">* To summerize the objective of data wrangling process, we have to split it into four phases.\n",
        "\n",
        ">> 1. Null/Missing value handling\n",
        ">> 2. Duplicate value dectection\n",
        ">> 3. Data type correction & new feature creation\n",
        ">> 4. Outlier analysis\n",
        "\n",
        ">* In the first phase we have came to the coclusion that the data set is free form the null/missing values.\n",
        "\n",
        ">* In the second phase we conclude that there are no duplicates in the dataset\n",
        "\n",
        ">* In the third phase, we discover that the data type of the `Date` feature is string, and all the other feature having correct data types. So we have convert the data type of required feature to proper`YYYY-MM-DD` format. & added each part as new feature.\n",
        "\n",
        ">* In the last phase we have analyze the outlier in the dataset using boxplot and came to conclusion that, there are no outliers present in the dataset"
      ],
      "metadata": {
        "id": "B23KcjWZgh6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***\n",
        "\n",
        ">Data visualization is a powerful tool that enables us to make sense of complex datasets and communicate insights in a clear and engaging manner. By experimenting with different types of charts, we can gain a deeper understanding of the relationships between variables and uncover patterns and trends that may be difficult to discern from raw data alone. When done effectively, data visualization can also be used to tell a compelling story that helps stakeholders understand the key takeaways from a dataset. By combining data visualization and storytelling, we can create a narrative that not only conveys important insights but also resonates with the audience on an emotional level. Ultimately, the goal of data visualization is to create a visual representation of data that is both informative and impactful, enabling us to make informed decisions based on a deeper understanding of the data.\n",
        "\n",
        "> We follow the `UBM` rule to analyze the patterns in the datset.\n",
        ">> Univariate Analysis<br>\n",
        ">> Bivariate Analysis<br>\n",
        ">> Multivariate Analysis<br>"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ▶ Univarate Analysis\n",
        "\n",
        ">It involves the examination of one variable at a time.\n",
        "This type of analysis is useful for understanding the distribution of a single variable and identifying\n",
        "any patterns or trends in the data."
      ],
      "metadata": {
        "id": "9Qh6kB8zg2Cq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1. Using distplot checking the distribution of `Open` feature"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking distribution of 'Open' feature\n",
        "plt.figure(figsize = (10,5))\n",
        "sns.distplot(df['Open'])\n",
        "plt.axvline(np.mean(df['Open']),color='r', linestyle='--')\n",
        "plt.axvline(np.median(df['Open']),color='b', linestyle='--')"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Hence, from above plot we see that the distribution if rightly skewed.\n",
        "\n",
        "> <b>Let's apply the log transformation to the distribution.</b>\n",
        "\n",
        ">>The log transformation is, arguably, the most popular among the different types of transformations used to transform skewed data to approximately conform to normality. If the original data follows a log-normal distribution or approximately so, then the log-transformed data follows a normal or near normal distribution."
      ],
      "metadata": {
        "id": "YbIKbTpnhasB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply log transformation it has right skewed distribution\n",
        "plt.figure(figsize = (10,5))\n",
        "sns.distplot(np.log(df['Open']))"
      ],
      "metadata": {
        "id": "iUk-cTSphe4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Our objective is to understand the distribution of the data. To fulfill this requirement, a distplot with a KDE curve is the best visualization, as it provides a better understanding of the feature's observations."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ">* The observations are right-skewed.\n",
        ">* After performing a log transformation, the distribution becomes normal."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Not much specific in this context"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 - Using distplot checking the distribution of `High` feature"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking distribution of 'High' variable\n",
        "plt.figure(figsize = (10,5))\n",
        "sns.distplot(df['High'])\n",
        "plt.axvline(np.mean(df['High']),color='r', linestyle='--')\n",
        "plt.axvline(np.median(df['High']),color='b', linestyle='--')"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Here, also the distribution is positively skewed.\n",
        "\n",
        "> Let's apply the log transformation."
      ],
      "metadata": {
        "id": "mPNOpCWgi0Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply log transformation it has right skewed distribution\n",
        "plt.figure(figsize = (10,5))\n",
        "sns.distplot(np.log(df['High']))"
      ],
      "metadata": {
        "id": "jpuYBjqAizuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* As our objective is to get the idea of the distribution. So to fullfill this requirement the distplolt with kde curve is best visulization, as it give the better idea of feature's observations."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* The observations are right skewed.<br>\n",
        ">* After doing the log transformation the destribution becomes the normal"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Not much specific in this context"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 - Using distplot checking the distribution of `Low` feature\n"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking distribution of 'Low' variable\n",
        "plt.figure(figsize = (10,5))\n",
        "sns.distplot(df['Low'])\n",
        "plt.axvline(np.mean(df['Low']),color='r', linestyle='--')\n",
        "plt.axvline(np.median(df['Low']),color='b', linestyle='--')"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> It is a right skewed distribution<br>\n",
        "> Let's apply the log transformation."
      ],
      "metadata": {
        "id": "sT5ZH8bejqFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply log transformation it has right skewed distribution\n",
        "plt.figure(figsize = (10,5))\n",
        "sns.distplot(np.log(df['Low']))"
      ],
      "metadata": {
        "id": "SOR0WYnOjnmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* As our objective is to get the idea of the distribution. So to fullfill this requirement the distplolt with kde curve is best visulization, as it give the better idea of feature's observations.\n",
        "\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* The observations are right skewed.<br>\n",
        ">* After doing the log transformation the destribution becomes the normal"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Not much specific in this context"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 - Using distplot checking the distribution of `Close` feature"
      ],
      "metadata": {
        "id": "OoI_jbzTkxxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking distribution of 'Close' variable\n",
        "plt.figure(figsize = (10,5))\n",
        "sns.distplot(df['Close'])\n",
        "plt.axvline(np.mean(df['Close']),color='r', linestyle='--')\n",
        "plt.axvline(np.median(df['Close']),color='b', linestyle='--')"
      ],
      "metadata": {
        "id": "m4eiy7xxkxxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> It's a right skewed distribution"
      ],
      "metadata": {
        "id": "kgCvNDLYWhGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply log transformation it has right skewed distribution\n",
        "plt.figure(figsize = (10,5))\n",
        "sns.distplot(np.log(df['Close']))"
      ],
      "metadata": {
        "id": "o6o_oj7JWsiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "FT5ow_ptkxxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* As our objective is to get the idea of the distribution. So to fullfill this requirement the distplolt with kde curve is best visulization, as it give the better idea of feature's observations."
      ],
      "metadata": {
        "id": "qi0kBwCCkxxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "etAYVlTbkxxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* The observations are right skewed.<br>\n",
        ">* After doing the log transformation the destribution becomes the normal"
      ],
      "metadata": {
        "id": "dcCTiMrZkxxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "kGUlGlgSkxxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Not much specific in this context"
      ],
      "metadata": {
        "id": "j85VhNHIkxxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 - Average price trends over months using line plot."
      ],
      "metadata": {
        "id": "CNj6zL5-x8ZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lineplot to check the  average price trends over months\n",
        "fig = plt.figure(figsize=(12,15))\n",
        "\n",
        "df.groupby('Month').aggregate({'Open' : 'mean', 'High' : 'mean', 'Low' : 'mean', 'Close' : 'mean'}).plot()\n",
        "\n",
        "plt.xticks(np.arange(13))\n",
        "plt.ylabel('Average stock price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u_HCU4BJx8ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "rVeE6pgwx8ZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">*  Line plot are very goods when it comes to analyzing the trends over stipulated time. Also gives the better idea of trend."
      ],
      "metadata": {
        "id": "S_8m-0PCx8ZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "-dwmSWFNx8ZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* The High phase of the stock is between Feb-Mar.\n",
        "\n",
        ">* After September, stock prices plummet."
      ],
      "metadata": {
        "id": "tsVXK4tmx8ZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3fCQnVPux8ZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Open and high follow the same path. It means that  closing price donsn't affect the opening pric of the stock"
      ],
      "metadata": {
        "id": "ew7dqDk7x8ZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 - Average price trends over querters using line plot."
      ],
      "metadata": {
        "id": "gPtaj0s5yL8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Barplot for average trends over quarter\n",
        "ax = df.groupby('Quarter').aggregate({'Open' : 'mean', 'High' : 'mean', 'Low' : 'mean', 'Close' : 'mean'}).plot.bar()\n",
        "ax.legend(loc='lower right')\n",
        "plt.ylabel('Average stock price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6TVDo1XFyL8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "GBiqunkcyL8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* As we only have limited number of x-axis values, specifically 4. So it is better to use bar chart instead of line."
      ],
      "metadata": {
        "id": "PH05yH0HyL8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "jXXk61KZyL8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* All the feature values in $4^{th}$ quarther is less as compair to other quarthers."
      ],
      "metadata": {
        "id": "SS4msdGFyL8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "iBebsfugyL8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Not specific in this context."
      ],
      "metadata": {
        "id": "D4okjzdeyL8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 - Average price trends over each year using line plot."
      ],
      "metadata": {
        "id": "BwGNzHKjyd2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lineplot for average trends over each year\n",
        "fig = plt.figure(figsize=(15,18))\n",
        "count=1\n",
        "for i in df['Year'].unique() :\n",
        "    plt.subplot(8, 2, count)\n",
        "    plt.xlabel(i, fontsize = 12)\n",
        "    plt.plot(df['Date'][df['Year'] == i] , df['Open'][df['Year'] == i] , label = 'Open')\n",
        "    plt.plot(df['Date'][df['Year'] == i] , df['High'][df['Year'] == i] , label = 'High')\n",
        "    plt.plot(df['Date'][df['Year'] == i] , df['Low'][df['Year'] == i], label = 'Low')\n",
        "    plt.plot(df['Date'][df['Year'] == i] , df['Close'][df['Year'] == i], label = 'Close')\n",
        "    plt.legend()\n",
        "    count = count + 1\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "Sf0KWdu8yd2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "2ZTZQpWkyd2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* To visulize the trend over certain time period the line chart is the best option."
      ],
      "metadata": {
        "id": "tNPo_tXtyd2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "5stELz6Nyd2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> * There is general upward trend in the yes bank stock.\n",
        "\n",
        "> *  There are two exception to this trend in the year <b>2008</b> & <b>2019</b>.\n",
        "\n",
        "> * The important factor for exception is <b>Market Sentiment</b>.\n",
        "\n",
        ">* The Highest gain recorded in <b>2014</b>, which is approx. <b>328.57%</b> in just <b>10 months</b>."
      ],
      "metadata": {
        "id": "xrASpaqyyd2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "er1SymyIyd2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Apart form few expections, the stock are up and high. which means that the bank performing well in all economic indicators and holds true for investors.\n",
        "\n",
        ">* Outside news and quarterly result have a significat impact on the stock price of the bank. As we seen that in <b>2008 due to *Rana Kappor case* and in 2019 due to *COVID-19 Pandamic* the stock get nose dive.</b>"
      ],
      "metadata": {
        "id": "Ai2KSTP9yd2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ▶ Bivariate Analysis\n",
        "\n",
        ">It involves the examination of the relationship between two variables. This type of analysis is useful for identifying any correlations or associations between the variables being analyzed.\n"
      ],
      "metadata": {
        "id": "sd_7f18uX-9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 - reg plot between `Open-Close`feature"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reg plot to check the relation between 'Open' and the dependent variable 'Close'\n",
        "plt.figure(figsize = (8,5))\n",
        "sns.regplot(x=df['Open'], y=df['Close'])"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Regplot is very good at visualizing linear relation between two variables. And also gives us better understanding of how data points are packed together"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* There is strong linear relationship between `Open` and `Close` feature."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* The given feature may highly contribute to the dependent feature, ofcourse it can be inviscate further."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 -  reg plot between `High-Close` feature"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reg plot to check the relation between 'High' and the dependent variable 'Close'\n",
        "plt.figure(figsize = (8,5))\n",
        "sns.regplot(x=df['High'], y=df['Close'])"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Regplot is very good at visualizing linear relation between two variables. And also gives us better understanding of how data points are packed together"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* There is strong linear relationship between `High` and `Close` feature."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* The given feature may highly contribute to the dependent feature, ofcourse it can be inviscate further."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 - reg plot between `Low-Close` feature"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reg plot to check the relation between 'Low' and the dependent variable 'Close'\n",
        "plt.figure(figsize = (8,5))\n",
        "sns.regplot(x=df['Low'], y=df['Close'])"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Regplot is very good at visualizing linear relation between two variables. And also gives us better understanding of how data points are packed together"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* There is strong linear relationship between `Low` and `Close` feature."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* The given feature may highly contribute to the dependent feature, ofcourse it can be inviscate further."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 - reg plot between `Year-Close` feature"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reg plot to check the relation between 'Year' and the dependent variable 'Close'\n",
        "plt.figure(figsize = (8,5))\n",
        "sns.regplot(x=df['Year'], y=df['Close'])"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Regplot is very good at visualizing linear relation between two variables. And also gives us better understanding of how data points are packed together"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Price is going up with year, unless something unusual hppens."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Unless any exteranal factor influence, the prices show the a positive relationships between the features."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ▶ Multivariate Analysis\n",
        "\n",
        ">It involves the examination of more than two variables at the same time. This type of analysis is useful for understanding the relationships between multiple variables and identifying any patterns or trends that may exist."
      ],
      "metadata": {
        "id": "kCVJHLEs5Pw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 - Lineplot to check Price trend ('Open', 'High', 'Low', 'Close')"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Lineplot for visualizing price trend from 2005 to 2020\n",
        "plt.figure(figsize = (12,6))\n",
        "plt.plot(df['Date'] , df['Open'] , label = 'open')\n",
        "plt.plot(df['Date'] , df['High'] , label = 'High')\n",
        "plt.plot(df['Date'] , df['Low'], label = 'Low')\n",
        "plt.plot(df['Date'] , df['Close'], label = 'Close')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Line chart is good for visualizing price trends over time series data."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We observe upword trend till 2018.\n",
        "\n",
        ">* Afterword of 2018, stock prices are down sharply."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Due to COVID-19 and Rana Koopur case the stock prices are impacted badly, and investor sectiment also gets negative towards bank."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Stackbar chart for each independent feature wrt year"
      ],
      "metadata": {
        "id": "aLttyvBYicL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pivot table to aggregate Open, High, Low, and Close by Year\n",
        "pivot = df.pivot_table(values=['Open', 'High', 'Low', 'Close'], index='Year', aggfunc='mean')\n",
        "\n",
        "# Create stacked bar chart with adjusted figure size\n",
        "pivot.plot(kind='bar', stacked=True, figsize=(12,6))\n",
        "\n",
        "# Customize chart labels and title\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Price')\n",
        "plt.title('Open, High, Low, and Close by Year')\n",
        "plt.legend(['Open', 'High', 'Low', 'Close'], loc='upper left')\n",
        "\n",
        "# Display chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cMAq-0-BicL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "MLcoMXJWicL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Stacked bar charts are often used for multivariate analysis as they allow you to compare multiple variables across different categories.\n",
        "\n",
        ">* A stacked bar chart displays the total amount or proportion of a variable broken down by subcategories and displayed as bars stacked on top of each other.\n",
        "\n",
        ">* Stacked bar charts are useful for comparing the relative size or proportion of different subcategories within each category, as well as comparing the total size or proportion of categories across different variables."
      ],
      "metadata": {
        "id": "Pry6HOTsicL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "T_Fv8w4cicL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Variable distribution for each year is some what same.\n",
        "\n",
        ">* The pick year is 2017 and lowest is at 2005."
      ],
      "metadata": {
        "id": "Ewq-vebaicL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "TiieUfuHicL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* There is lot's of selling taking place after 2019, due to various factors. which contribute the negative growth of the bank."
      ],
      "metadata": {
        "id": "bA7NCMORicL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Candlestick chart of price movement"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Candlestick chart for visualizing overall stock price movement.\n",
        "fig = go.Figure(data=[go.Candlestick(x=df['Date'],\n",
        "                open=df['Open'],\n",
        "                high=df['High'],\n",
        "                low=df['Low'],\n",
        "                close=df['Close'])])\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* The candlestick charts are very common for analyzing the movement of the stock. Also it gives us better and details analysis of the stock price movement."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We wil see a bull run form 2014 to the first-quarter of 2019.\n",
        "\n",
        ">* The stock hit seriously due to the COVID-19 Pandamic.\n",
        "\n",
        ">* Due to rana koopur case the banking financial got hit, and due to negative sectiment in the market the prices gets drop.\n",
        "\n"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* As we don't control the outside factors for imacting the financial of the company. but we do control the companies interal core working."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 16 - Headmap for \"min\", \"max\", \"mean\" prices of each year"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# heatmap for visualizing min, max and mean prices for each year\n",
        "Year_prices_df = df.groupby('Year').aggregate({'Open' : [min, max, 'mean'], 'High' : [min, max, 'mean'], 'Low' : [min, max, 'mean'], 'Close' : [min, max, 'mean']})\n",
        "\n",
        "# plotting the heatmap\n",
        "plt.figure(figsize = (15,10))\n",
        "sns.heatmap(Year_prices_df , annot = True, cmap = 'coolwarm', fmt='.2f')\n",
        "plt.xlabel('Min-Max-Mean')"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Heatmap is best at visulaizing low and high values , where value is low , where it is high, where it is little low, where it is little high, where it is median something like this."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* All time high was 404, it was in 2018.\n",
        ">* All time low was 5.55, it was in 2020.\n",
        ">* Here also we can price drop after 2018."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Not specific in the this context"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 17 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the heatmap using sns.heatmap()\n",
        "plt.subplots(figsize=(10, 8))\n",
        "sns.heatmap(df.corr(), annot=True, linewidth=.5, cmap='coolwarm')"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Using heatmap we better understand the correlation between variables."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Independent variables shares a very high correlation among themself, so we have a problem of multicollinearity here.\n",
        "\n",
        ">* The Dependent variable `Close` shares a very high correlation with other independent variables such as `Open`, `High`, `Low`\n",
        "\n",
        ">* The only variables which having less correlation is `Year`, `Month` and `Quarter`."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 18 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization\n",
        "sns.pairplot(df , diag_kind = 'kde')"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Pairs plots are a powerful tool to quickly explore distributions and relationships in a dataset.\n",
        "\n",
        ">* A pairs plot allows us to see both distribution of single variables and relationships between two variables.\n",
        "\n",
        ">* Pair plots are a great method to identify trends for follow-up analysis."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Open , high, low and close variables has skewed diatribution.\n",
        "\n",
        ">* Open , high, low and close variables are showing strong linear pattern with each other."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***\n",
        "<hr>"
      ],
      "metadata": {
        "id": "2WTjHt85dgNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "<center><img src=\"https://github.com/ashish-mali/Regression-Yes-Bank-Stock-Closing-Price-Prediction/blob/main/Images/hypothesis_testing_title.png?raw=true\" alt=\"Hypothesis_Testing_Title\" width=\"800\" height=\"400\"></center>\n",
        "<hr>\n",
        "<br>\n",
        "\n",
        "\n",
        ">Hypothesis testing in machine learning is a statistical method used to determine whether a hypothesis about a population is true or false based on a sample of data. It is an important part of the model-building process in machine learning because it allows us to validate or reject assumptions about the data and make statistical inferences about the population.\n",
        "\n",
        ">The process of hypothesis testing involves the following steps:\n",
        "\n",
        ">>1. Formulate a null hypothesis and an alternative hypothesis. The null hypothesis represents the status quo or the default assumption, while the alternative hypothesis represents the new or alternative assumption.\n",
        "\n",
        ">>2. Choose a significance level (also called alpha), which is the probability of rejecting the null hypothesis when it is actually true. A common significance level is 0.05, which means that we are willing to accept a 5% chance of making a Type I error (rejecting the null hypothesis when it is true).\n",
        "\n",
        ">>3. Collect a sample of data from the population and calculate a test statistic that measures the difference between the sample data and the null hypothesis.\n",
        "\n",
        ">>4. Determine the p-value, which is the probability of observing a test statistic as extreme or more extreme than the one calculated from the sample data, assuming the null hypothesis is true.\n",
        "\n",
        ">>5. Compare the p-value to the significance level. If the p-value is less than or equal to the significance level, we reject the null hypothesis and conclude that the alternative hypothesis is supported by the data. Otherwise, we fail to reject the null hypothesis and conclude that there is not enough evidence to support the alternative hypothesis.\n",
        "\n",
        "Hypothesis testing is used in various applications in machine learning, such as feature selection, model comparison, and parameter tuning. It helps us make data-driven decisions by providing a framework for testing hypotheses and drawing conclusions based on the evidence presented by the data.\n"
      ],
      "metadata": {
        "id": "CTwGpvOPgbbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Y8ZdB7qmdgNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://github.com/ashish-mali/Regression-Yes-Bank-Stock-Closing-Price-Prediction/blob/main/Images/steps_in_hypothesis_testing.png?raw=true\" alt=\"Steps In Hypothesis_Testing_Title\" width=\"auto\" height=\"auto\"></center>"
      ],
      "metadata": {
        "id": "N0p_NFlCtf3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* <b>Statement-01:-</b> The average closing price of the stock is higher in the $1^{st}$ quarter of the year compared to the $3^{rd}$ quarter.\n",
        "\n",
        ">* <b>Statement-02:-</b> The average opening price of the stock is higher on days when the market closes with a gain (i.e., the closing price is higher than the opening price) compared to days when the market closes with a loss.\n",
        "\n",
        ">* <b>Statement-03:-</b> The average return of the stock in 2019 is less than the average return of the stock in 2020."
      ],
      "metadata": {
        "id": "1N-4NqCFdgNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1\n",
        "\n",
        ">The average closing price of the stock is higher in the $1^{st}$ quarter of the year compared to the $3^{rd}$ quarter.\n"
      ],
      "metadata": {
        "id": "CrBD4OgVdgNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "RDshy7IVdgNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* <b>Null Hypothesis ($H_o$) :</b> $μ_1$ ${-}$ $μ_2$ ${=}$ ${0}$\n",
        "\n",
        ">* <b>Alternate Hypothesis ($H_a$) :</b> $μ_1$ ${>}$ $μ_2$\n",
        "\n",
        ">Where :\n",
        "\n",
        ">> $μ_1$ ${=}$ Sample mean of the stock's closing price in the $1^{st}$ quarter\n",
        "\n",
        ">> $μ_2$ ${=}$ Sample mean of the stock's closing price in the $3^{rd}$ quarter\n",
        "<br>\n",
        "\n",
        ">* Test Type :- Right-tail test\n",
        "\n",
        ">* Singificance Level = 0.05"
      ],
      "metadata": {
        "id": "H7ndqluGdgNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "ox9CjttIdgNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Statement 1: average closing price higher in Q1 compared to Q2\n",
        "Q1_close = df[df['Quarter'] == 1]['Close']\n",
        "Q3_close = df[df['Quarter'] == 2]['Close']\n",
        "\n",
        "# Perform a paired t-test to check if the difference is significant\n",
        "t_stat, p_val = ttest_rel(Q1_close, Q3_close)\n",
        "\n",
        "# Checking if the null hypothesis is rejected or not\n",
        "if p_val < 0.05:\n",
        "    print(f'Null Hypothesis Is Rejected As P-Value Is:-', p_val)\n",
        "else:\n",
        "    print(f'Failed To Reject Null Hypothesis As P-Value Is:-', p_val)"
      ],
      "metadata": {
        "id": "sEPpCswfdgNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "ix7EXaAvdgNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We used a paired t-test because we are comparing the means of two related samples (i.e., the same stock in different quarters)."
      ],
      "metadata": {
        "id": "Bo8_AcnzdgNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "iUtvFNI6dgNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* The paired t-test is appropriate when the same individuals are being compared across two time points or conditions, and the samples are assumed to be normally distributed and have equal variances."
      ],
      "metadata": {
        "id": "4PzxeG50dgNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2\n",
        "\n",
        ">* The average opening price of the stock is higher on days when the market closes with a gain (i.e., the closing price is higher than the opening price) compared to days when the market closes with a loss."
      ],
      "metadata": {
        "id": "pcNnAcdBdgNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "nn2zJ68qdgNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* <b>Null Hypothesis ($H_o$) :</b> $μ_{gain}$ ${-}$ $μ_{loss}$ ${=}$ ${0}$\n",
        "\n",
        ">* <b>Alternate Hypothesis ($H_a$) :</b> $μ_{gain}$ ${>}$ $μ_{loss}$\n",
        "\n",
        ">Where :\n",
        "\n",
        ">> $μ_{gain}$ ${=}$ Sample mean of the stock's opening price on days when the market closes with a gain.\n",
        "\n",
        ">> $μ_{loss}$ ${=}$ Sample  mean of the stock's opening price on days when the market closes with a loss.\n",
        "\n",
        ">* Test type : Right Tailed test\n",
        "\n",
        ">* Singificance Level = 0.05"
      ],
      "metadata": {
        "id": "D4P83mLKdgNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "RKqGeCFxdgNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Statement 2: average opening price higher on gain days compared to loss days\n",
        "gain_open = df[df['Close'] > df['Open']]['Open']\n",
        "loss_open = df[df['Close'] < df['Open']]['Open']\n",
        "\n",
        "# Perform a independent t-test to check if the difference is significant\n",
        "t_stat, p_val = ttest_ind(gain_open, loss_open)\n",
        "\n",
        "# Checking if the null hypothesis is rejected or not\n",
        "if p_val < 0.05:\n",
        "    print(f'Null Hypothesis Is Rejected As P-Value Is:-', p_val)\n",
        "else:\n",
        "    print(f'Failed To Reject Null Hypothesis As P-Value Is:-', p_val)"
      ],
      "metadata": {
        "id": "5RRZOxzzdgNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "t8SrzZCTdgNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We used an independent samples t-test because, we where comparing the means of two unrelated samples (i.e., different stocks on gain and loss days)."
      ],
      "metadata": {
        "id": "-kzRgSPRdgNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "mPuanE-ldgNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* The independent samples t-test is appropriate when the samples are assumed to be normally distributed and have equal variances."
      ],
      "metadata": {
        "id": "tR8vvVBMdgNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3\n",
        "\n",
        ">The average return of the stock in 2019 is less than the average return of the stock in 2020."
      ],
      "metadata": {
        "id": "14edKR07dgNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "lJm_b2XLdgNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* <b>Null Hypothesis ($H_o$) :</b> $μ_{(2019)}$ ${-}$ $μ_{(2020)}$ ${=}$ ${0}$\n",
        "\n",
        ">* <b>Alternate Hypothesis ($H_a$) :</b> $μ_{(2019)}$ ${<}$ $μ_{(2020)}$\n",
        "\n",
        ">Where :\n",
        "\n",
        ">> $μ_{(2019)}$ ${=}$ Sample mean return of the stock in 2019.\n",
        "\n",
        ">> $μ_{(2020)}$ ${=}$ Sample  mean return of the stock in 2020.\n",
        "\n",
        ">* Test type : Left Tailed test\n",
        "\n",
        ">* Singificance Level = 0.05"
      ],
      "metadata": {
        "id": "u-0ppN7edgNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "FEouoQdYdgNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Statement 3: the average return in 2019 is less than in 2020\n",
        "# get the data for the two years\n",
        "data_2019 = df.loc[df['Year'] == 2019, 'Close'].pct_change().dropna()\n",
        "data_2020 = df.loc[df['Year'] == 2020, 'Close'].pct_change().dropna()\n",
        "\n",
        "# perform a two-sample t-test to check if the difference is significant\n",
        "t_stat, p_val = ttest_ind(data_2019, data_2020, alternative='less')\n",
        "\n",
        "# check if the null hypothesis is rejected or not\n",
        "if p_val < 0.05:\n",
        "    print(f'Null Hypothesis Is Rejected As P-Value Is:-', p_val)\n",
        "else:\n",
        "    print(f'Failed To Reject Null Hypothesis As P-Value Is:-', p_val)"
      ],
      "metadata": {
        "id": "tGPo4BnudgNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "i8dSq9BxdgNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* An independent samples t-test was used since we are comparing the means of two separate groups."
      ],
      "metadata": {
        "id": "bXQEuonfdgNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "hv51gq_5dgNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* The t-test is a commonly used statistical test for comparing means, and is appropriate when the sample size is relatively small (less than 30) and the population standard deviation is unknown."
      ],
      "metadata": {
        "id": "DHqZCm8TdgNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***\n",
        "<hr>"
      ],
      "metadata": {
        "id": "6MjnT1NWdgNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "jyPcWhkWdgNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "r_EoVDgZdgNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "ZdP6ifHYdgNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">The dataset does't contain the null/missing values."
      ],
      "metadata": {
        "id": "Y-MQfel_dgNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "a69J0CEgdgNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "def identify_outliers(df):\n",
        "    numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
        "    z_scores = df[numerical_columns].apply(lambda x: (x - x.mean()) / x.std())\n",
        "    outliers = np.abs(z_scores) > 3\n",
        "    return outliers\n",
        "\n",
        "def remove_outliers(df):\n",
        "    outliers = identify_outliers(df)\n",
        "    if outliers.any().any():\n",
        "        df_with_outliers = df[outliers.any(axis=1)]\n",
        "        return df_with_outliers\n",
        "    else:\n",
        "        return \"No outliers detected\""
      ],
      "metadata": {
        "id": "FpQjAj3udgNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_outliers(df)"
      ],
      "metadata": {
        "id": "TkxY-BPtinXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "xvEl6JExdgNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">There where no outliers in the dataset."
      ],
      "metadata": {
        "id": "6xSG4ekVdgNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding\n",
        "\n",
        ">Categorical Encoding is a process where we transform categorical data into numerical data. There are many categorical encodings techniques avaliable, which to apply is depends upon the context of the problem.\n",
        "\n",
        ">The categorical data type is data divided by category or group — for example, Gender, Education, and Birth Place. Each element in the categorical data is used to group information on specific labels, different from numerical data where the numerical data information is in numbers. The problem with the unprocessed categorical data is the machine learning issue where many models cannot accept categorical data. This issue is why we need Categorical Encoding."
      ],
      "metadata": {
        "id": "_92C4OAsdgNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a look at our correlation heatmap\n",
        "plt.figure(figsize = (15,6))\n",
        "sns.heatmap(df.corr(), annot = True, cmap = 'coolwarm')"
      ],
      "metadata": {
        "id": "B-aU_HxWk9pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# get dummies variable for month feature\n",
        "df = pd.get_dummies(df,columns = ['Month'], prefix='month')"
      ],
      "metadata": {
        "id": "fDJxV2YudgNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "AgbBD41JdgNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "><b>One-Hot Encoding or OHE</b> is a technique that transforms all the elements on a categorical column into new columns represented by 0 or 1 (binary values) to signify the presence of the category value.\n",
        "\n",
        ">we will be using the Pandas package using a function called <code>get_dummies</code> to do the OHE."
      ],
      "metadata": {
        "id": "faTon7qmdgNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)\n",
        "\n",
        "(Note :  We omit this step as there is no textual data pre-processing involved.)"
      ],
      "metadata": {
        "id": "jrQvcS0adgNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "A-KhLxIWdgNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "DJUZnfpRdgNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "zAc3mQAAdgNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "sUgoz3o1dgNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "w9fAtvQRdgNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "YXBYu0oPdgNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "9VigawjldgNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "f7aZnxNSdgNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "NuSxnpyZdgNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "JMMABPnvdgNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "JLF-01V7dgNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "oDlidowPdgNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "0LPojy2AdgNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "ux7aSfnKdgNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "FRGmYNVZdgNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "GTarQ74TdgNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "vlOwq9KgdgNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "GGesfak2dgNh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "NvrC9Y7qdgNh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "ePRUkoCudgNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "yZnh-ZC7dgNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "Qs54dAj8dgNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "opq0DryndgNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "KPeg9GNtdgNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "4go7TLAWdgNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "ah6tHKKYdgNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "dRXthvNydgNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation heatmap\n",
        "plt.figure(figsize = (15,10))\n",
        "sns.heatmap(df.corr(), annot = True, cmap = 'coolwarm')"
      ],
      "metadata": {
        "id": "gM20p6zQqJOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Create new feature by taking averege of (open, low, high) as they are highly correalation with each other\n",
        "df['OHL'] = df[['Open', 'High', 'Low']].mean(axis=1).round(2)\n",
        "\n",
        "# Show new df\n",
        "df.head()"
      ],
      "metadata": {
        "id": "bP9VvBV6dgNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Let's check the distribution of the `OHL` feature."
      ],
      "metadata": {
        "id": "6ZVRX3peqwUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the distribution 'OHL' feature\n",
        "sns.distplot(df['OHL'])"
      ],
      "metadata": {
        "id": "l0HTzFtcq4rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">After plotting the distribution of `HOL` feature, we came to the conclusion that the distribution if the <b>positively skewed</b>.\n",
        "\n",
        ">We must have to perform the <b>log() transformation</b>, in order to get the data normally distributed.\n",
        "\n"
      ],
      "metadata": {
        "id": "pf4UCo3rrIeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying log transform on 'OHL\n",
        "sns.distplot(np.log(df['OHL']))"
      ],
      "metadata": {
        "id": "H4JNFt-csb_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Now, plotting the regplot to check the linearity between `OHL` and Dependable variable."
      ],
      "metadata": {
        "id": "x72QiaoTsxpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regplot 'OHL' vs 'close'\n",
        "plt.figure(figsize = (8,5))\n",
        "sns.regplot(x=df['OHL'], y=df['Close'])\n",
        "plt.title('OHL vs Close')"
      ],
      "metadata": {
        "id": "cv-ybM_gtP0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Seems to be a very high corelation between these two features."
      ],
      "metadata": {
        "id": "Qz00U1_PtWh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "lDVBIaK0dgNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "><b>Variance Inflation Factor (VIF) :</b>\n",
        "\n",
        ">>The Variance Inflation Factor (VIF) technique from the Feature Selection Techniques collection is not intended to improve the quality of the model, but to remove the autocorrelation of independent variables.\n",
        "\n",
        ">>Collinearity is the state where two variables are highly correlated and contain similar information about the variance within a given dataset."
      ],
      "metadata": {
        "id": "kg90saIzujYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VIF\n",
        "def calculate_vif(X):\n",
        "\n",
        "  # calculating VIF\n",
        "  vif =pd.DataFrame()\n",
        "  vif[\"variables\"] = X.columns\n",
        "  vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X. shape[1])]\n",
        "\n",
        "  return(vif)"
      ],
      "metadata": {
        "id": "xJWGDVO1vbJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_vif(df[[i for i in df.describe().columns if i not in ['Date', 'Close']]])"
      ],
      "metadata": {
        "id": "eZ0u6_iUvlvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* The VIFs are very high so we have to drop certain feature in order to reduce the autocorrelation."
      ],
      "metadata": {
        "id": "Nx8DZTfTyTyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "selected_features = [ 'Year','OHL','month_1', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n",
        "       'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12', 'Close']\n",
        "\n",
        "# new df\n",
        "selected_df = df[selected_features]"
      ],
      "metadata": {
        "id": "6lt_myYKdgNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let look at correlation heatmap of selected df\n",
        "plt.figure(figsize = (15,10))\n",
        "sns.heatmap(selected_df.corr(), annot = True, cmap = 'coolwarm')"
      ],
      "metadata": {
        "id": "xCuWrvoWwXoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "mm85bNT2dgNq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">1. Feature engineering  \n",
        ">>* Created a new feature 'OHL' feature from 'Open', 'High' and 'Low' feature as they were had strong correlation with eachother as well as dependent variable 'Close'.\n",
        ">>* Created dummy variables for 'Month' feature.\n",
        "<br>\n",
        "\n",
        ">2. Multicollinearity\n",
        ">>* Droped 'Open', 'High' and 'Low' feature as they were had strong correlation with each other also have very high VIFs.\n",
        ">>* Droped 'quarter' as it was highly correalated with 'month'.\n",
        "\n",
        ">3. Correaltion\n",
        ">>* Selected features based on their correlation with dependent variable."
      ],
      "metadata": {
        "id": "xWaLSPdFdgNq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "yjIhzSHPdgNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* `OHL` which is average of `Open`, `High` and `Low` is most important as it is highly correlated with dependent variable.\n",
        "\n",
        "\n",
        ">* `Year` and `Month` are also important features as it is a timeseries dataset and also has decent correlation with dependent variable."
      ],
      "metadata": {
        "id": "1DWwNgIAdgNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Transformation"
      ],
      "metadata": {
        "id": "mI-C8HEydgNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?\n",
        "\n",
        ">* Data transformation is need on `OHL`, as you can see in above plot, it is a right skewed distribution. so we are going to use `log()` transformation."
      ],
      "metadata": {
        "id": "BDUw9PSVdgNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First creating the set of  independent and dependent variables\n",
        "X = selected_df.drop(labels=['Close'], axis=1)\n",
        "\n",
        "y = selected_df['Close']"
      ],
      "metadata": {
        "id": "xrD-iwoM3lAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking distribution 'OHL' feature\n",
        "sns.distplot(X['OHL'])"
      ],
      "metadata": {
        "id": "t4P-TWmC30fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# log transformation\n",
        "X['OHL'] = np.log(X['OHL'])"
      ],
      "metadata": {
        "id": "F3o1dRS9dgNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Data Scaling\n",
        "\n",
        ">Feature Scaling is a technique to standardize the independent features present in the data in a fixed range. It is performed during the data pre-processing to handle highly varying magnitudes or values or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values, higher and consider smaller values as the lower values, regardless of the unit of the values."
      ],
      "metadata": {
        "id": "8kVv5KiidgNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "scaler = StandardScaler()\n",
        "x = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "D5gikMWwdgNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "\n",
        ">We used <b>StandardScaler</b> which is used to resize the distribution of values ​​so that the mean of the observed values ​​is 0 and the standard deviation is 1."
      ],
      "metadata": {
        "id": "KqW-Rt78dgNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Dimesionality Reduction\n",
        "\n",
        ">Dimensionality reduction is the process of reducing the number of features in a dataset while retaining as much information as possible."
      ],
      "metadata": {
        "id": "-w2h6D_CdgNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "IlbLA7bydgNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* The dimensionality reduction is not required as the dataset already have few features."
      ],
      "metadata": {
        "id": "7KjUxMgEdgNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "Lnb2iZpbdgNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "yCFIntpqdgNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "cE2QH9qMdgNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Data Splitting"
      ],
      "metadata": {
        "id": "YFrad6GXdgNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1)"
      ],
      "metadata": {
        "id": "Jy2b35eIdgNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "mtCEvV3adgNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">We choose $80%$${/}$$20%$ splitting strategy to preserve the continuity in features. The first 80% for training and the rest 20% for testing."
      ],
      "metadata": {
        "id": "abBSDwl7dgNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "4WqcNzRVdgNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "5OsgBY2LdgNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Imbalanced data refers to those types of datasets where the target class has an uneven distribution of observations, i.e one class label has a very high number of observations and the other has a very low number of observations.\n",
        "\n",
        ">We don't have the imbalanced dataset"
      ],
      "metadata": {
        "id": "2vC_rTYHdgNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "RE_WXqeSdgNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "XpCPOu_edgNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6YNsf17ddgNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***\n",
        "<hr>"
      ],
      "metadata": {
        "id": "o8VCF6jrdgNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Let's create some preliminary  functions which repetitively requaired during ML Implementation."
      ],
      "metadata": {
        "id": "7le1ilXND4-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating functions for model implementation\n",
        "\n",
        "# Function to check the predication of model\n",
        "def check_pred(model):\n",
        "\n",
        "    '''\n",
        "    Function to check the actual closing and predicated closing price.\n",
        "    '''\n",
        "\n",
        "    # Creating variable takes the predicted values\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    # Creating the DataFrame of test and train Dataset\n",
        "    train_Dataset = pd.DataFrame(x_train,y_train)\n",
        "    test_Dataset = pd.DataFrame(y_test)\n",
        "    test_Dataset.rename(columns= {'Close' :'Actual Closing Price'}, inplace =True)\n",
        "\n",
        "    # Creating new column taking a predicated value\n",
        "    test_Dataset['Predicted Closing Price']= y_pred\n",
        "\n",
        "    # output the created dataframe\n",
        "    return test_Dataset.head()\n",
        "\n",
        "# Function to plot actual vs prediction\n",
        "def plot_actual_vs_pred(model, model_name = None, x_test = x_test, y_test = y_test):\n",
        "\n",
        "  #doc string\n",
        "  '''\n",
        "  function visualize actual vs predicted outputs\n",
        "  '''\n",
        "\n",
        "  # Predict on the model\n",
        "  y_pred = model.predict(x_test)\n",
        "  plt.plot(y_pred, label=\"Prediction\", linewidth=1.5)\n",
        "  plt.plot(y_test.values, label=\"Actual\", linewidth=1.5)\n",
        "\n",
        "\n",
        "  error = mean_absolute_error(y_test, y_pred)\n",
        "  plt.title(f\"{model_name} - Actual vs Predicted\")\n",
        "  plt.legend(loc=\"best\")\n",
        "  plt.tight_layout()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Define the time-series cross-validation object\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "\n",
        "# Function to calculate and plot Cross Validation Performance\n",
        "def plot_cv_perfomance(model, model_name = None, x_train = x_train, y_train = y_train, y_lim = (0, 1.1)):\n",
        "\n",
        "  #doc string\n",
        "  '''\n",
        "  a function to calculate and plot Cross Validation Performance\n",
        "  '''\n",
        "  cv_results = cross_validate(model, x_train, y_train, scoring='r2', cv = tscv, return_train_score=True)\n",
        "\n",
        "  # Plot mean accuracy scores for training and testing scores\n",
        "  plt.plot(cv_results['train_score'], label = \"Training Score\", marker='o', markersize=6, color = 'b', linewidth=2)\n",
        "  plt.plot(cv_results['test_score'], label = \"Cross Validation Score\", marker='o', markersize=6, color = 'g', linewidth=2)\n",
        "  plt.title(f\"{model_name} - Cross Validation Performance Plot\")\n",
        "\n",
        "  # plt.xlabel(\"K-fold cross validation\")\n",
        "  plt.ylabel(\"R2-score\")\n",
        "  plt.ylim(y_lim)\n",
        "  plt.tight_layout()\n",
        "  plt.legend(loc = 'best')\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "  print(f\"\\nThe CV test_Score : {cv_results['test_score']}\")\n",
        "  print(f\"Mean cross-validation score : {cv_results['test_score'].mean()}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# Function to plot coeficients\n",
        "def plot_coef(model,model_name = None):\n",
        "\n",
        "    '''\n",
        "    function to plot he coefficient of the model\n",
        "    '''\n",
        "    features = selected_df.drop('Close', axis = 1).columns\n",
        "    cofficients = model.coef_\n",
        "    plt.bar(features,cofficients)\n",
        "    plt.xticks(fontsize = 12, rotation=90)\n",
        "    plt.title(f'{model_name} - Coeficients')\n",
        "    plt.grid(True, axis='y')\n",
        "    plt.hlines(y=0, xmin=0, xmax=len(cofficients), linestyles='dashed')\n",
        "    plt.show\n",
        "\n",
        "\n",
        "# Function to check homoscedasticity\n",
        "def check_homoscedasticity(model, model_name = None, x_test = x_test, y_test = y_test):\n",
        "  '''\n",
        "  function to check homoscedasticity\n",
        "  '''\n",
        "  y_pred = model.predict(x_test)\n",
        "  residuals = y_test - y_pred\n",
        "  plt.scatter(y_pred, residuals)\n",
        "  plt.title(f'{model_name} - Residual Plot')\n",
        "  plt.xlabel('Predicted value')\n",
        "  plt.ylabel('residuals')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        " # Selected features\n",
        "features = selected_df.drop('Close', axis = 1).columns\n",
        "\n",
        "# Create dataframe for performance evaluation metrics\n",
        "p_metrics_df = pd.DataFrame({'metrics' : ['MAE', 'MSE', 'RMSE', 'MAPE', 'R2_score', 'adjusted_r2']})\n",
        "\n",
        "\n",
        "# Funtion to update p_metrics_df\n",
        "def update_p_metrics_df(model, model_name = None, x_test = x_test, y_test = y_test):\n",
        "  '''\n",
        "  funtion to update p_metrics_df dataFrame\n",
        "  '''\n",
        "  # Predict on the model\n",
        "  y_pred = model.predict(x_test)\n",
        "\n",
        "  # Evaluation metric scores\n",
        "  # Mean Absolute Error\n",
        "  MAE =  round(mean_absolute_error(y_test, y_pred), 4)\n",
        "\n",
        "  #Mean Squared Error\n",
        "  MSE = round(mean_squared_error(y_test, y_pred), 4)\n",
        "\n",
        "  # Root Mean Squared Error\n",
        "  RMSE = round(np.sqrt(MSE), 4)\n",
        "\n",
        "  # Mean Absolute Percentage Error\n",
        "  MAPE = round(mean_absolute_percentage_error(y_test, y_pred), 4)\n",
        "\n",
        "  # R Square\n",
        "  R2_score = round(r2_score(y_test, y_pred), 4)\n",
        "\n",
        "  # Adjusted R Square\n",
        "  n = y_test.size\n",
        "  p = features.size\n",
        "  adjusted_r2 = round(1-(1-R2_score)*(n-1)/(n-p-1), 4)\n",
        "\n",
        "  # Update p_metrics datframe\n",
        "  p_metrics_df[model_name] = [MAE, MSE, RMSE, MAPE, R2_score, adjusted_r2]\n",
        "  return p_metrics_df"
      ],
      "metadata": {
        "id": "Rm028fTCEzf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1\n",
        "\n",
        "### ♠ <b> Linear Regression</b>"
      ],
      "metadata": {
        "id": "kqheNsLDdgNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://github.com/ashish-mali/Regression-Yes-Bank-Stock-Closing-Price-Prediction/blob/main/Images/linear_regression.jpg?raw=true\" alt=\"LinearRegressionConcept\" width=\"1000\" height=\"500\"></center>"
      ],
      "metadata": {
        "id": "Whii490dL8RH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Linear regression is a supervised machine learning algorithm used for predicting a continuous output variable based on one or more input variables. It assumes a linear relationship between the input variables and the output variable.\n",
        "\n",
        ">* The goal of linear regression is to find the best-fit line that represents the relationship between the input variables and the output variable. The best-fit line is defined by a set of coefficients that minimize the sum of the squared errors between the predicted values and the actual values.\n",
        "\n",
        ">* In other words, linear regression tries to find a linear equation of the form\n",
        "<center>$y$ $=$ $β_0$ $+$ $β_1$$x_1$ $+$ $β_2$$x_2$ $+$ $...$ $+$ $β_n$$x_n$</center>\n",
        "\n",
        ">where,\n",
        "\n",
        ">><center>$y$ $=$ Dependent variable,<br>\n",
        ">>$x_1$, $x_2$, $...$ $x_n$ $=$ independent variables,<br>\n",
        ">>$β_0$ $=$ $y$ Intercept,<br>\n",
        ">>$β_1$, $β_2$, $...$ $β_n$ $=$ coefficients <br></center>\n",
        "\n",
        ">* Coefficients that represent the effect of each input variable on the output variable.\n",
        "\n",
        ">* The coefficients are estimated using a training set of data and an optimization algorithm such as ordinary least squares (OLS) or gradient descent. Once the coefficients are estimated, the model can be used to make predictions on new data.\n",
        "\n",
        ">* Linear regression is a simple and interpretable algorithm that can be used for a wide range of regression tasks. However, it assumes a linear relationship between the input variables and the output variable, and may not perform well when the relationship is non-linear."
      ],
      "metadata": {
        "id": "bjwRpS3QL6E_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "lr = LinearRegression()"
      ],
      "metadata": {
        "id": "UDiEM_YtdgNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Algorithm\n",
        "lr.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "AeIlzmA6P5Ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see the training and testing score\n",
        "print(f'Training Accuracy Score : {round(lr.score(x_train,y_train), 4)}')\n",
        "print(f'Testing Accuracy Score : {round(lr.score(x_test,y_test), 4)}')"
      ],
      "metadata": {
        "id": "bak0puG6Sirv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Training Score is higher than testing Score."
      ],
      "metadata": {
        "id": "3R_3Sj-kYk5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "check_pred(lr)"
      ],
      "metadata": {
        "id": "zzd1mWGoP5Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.figure(figsize = (10,5))\n",
        "plot_actual_vs_pred(model = lr, model_name = 'LinearRegression')"
      ],
      "metadata": {
        "id": "KSaOQTzHdePg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Some predicted values are going negative which should not be the case as price cannot be negative.\n",
        "\n",
        ">* At some points difference between actual and prediction is very high."
      ],
      "metadata": {
        "id": "2fkeQTXtdt3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ZSqEWwqgdgN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics - LinearRegression\n",
        "update_p_metrics_df(model = lr, model_name = 'LinearRegression' )"
      ],
      "metadata": {
        "id": "lXIWbamCdgN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* RMSE and MAE is relatively high.\n",
        "\n",
        ">* adjusted-R2 is lower than R2-score which indicates that the additional input variables are not adding value to the model."
      ],
      "metadata": {
        "id": "V5ni57anYyzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Let's check the linear regression coefficients."
      ],
      "metadata": {
        "id": "MBCuPR45jfbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The regression coefficients are:\", lr.coef_)\n",
        "\n",
        "print(\"The total regression coefficients are:\", len(lr.coef_))"
      ],
      "metadata": {
        "id": "JABzm4rzM9AB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, visualizing cofficients LinearRegression\n",
        "plt.figure(figsize = (10,5))\n",
        "plot_coef(model = lr, model_name = 'LinearRegression')"
      ],
      "metadata": {
        "id": "gQAhu6bxkPze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Among all the variables the `OHL` variable has highest coeficient value, it is contributing more to the model.\n",
        "\n",
        ">* When do the relative comparison to the `OHL`other variables are not adding much value to the model."
      ],
      "metadata": {
        "id": "-s_TVz-OkvEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Let's check the Homoscedasticity of the Linear Regression\n",
        "\n",
        ">>The assumption of homoscedasticity (meaning “same variance”) is central to linear regression models.  Homoscedasticity describes a situation in which the error term (that is, the “noise” or random disturbance in the relationship between the independent variables and the dependent variable) is the same across all values of the independent variables.  Heteroscedasticity (the violation of homoscedasticity) is present when the size of the error term differs across values of an independent variable.  The impact of violating the assumption of homoscedasticity is a matter of degree, increasing as heteroscedasticity increases."
      ],
      "metadata": {
        "id": "NUmytBaTl9bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for homoscedasticity - LinearRegression\n",
        "check_homoscedasticity(model = lr, model_name = 'LinearRegression')"
      ],
      "metadata": {
        "id": "4lp1azaOnfOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We can observe something like `V-shaped` pattern in residuals plot.\n",
        "\n",
        ">* From above observation we can say that the `PolyFit` model will better, ofcourse we have to check for overfitting."
      ],
      "metadata": {
        "id": "XbTuAz7Cn4i5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cross-Validation for better understanding of model performance"
      ],
      "metadata": {
        "id": "csy-4wbeFgyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Linear Regression Cross Validation Performance Plot and cv r2-score\n",
        "plt.figure(figsize = (10,5))\n",
        "plot_cv_perfomance(model = lr, model_name = 'LinearRegression')"
      ],
      "metadata": {
        "id": "8SlmoZggIfAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">*  We can expect the model accuracy as low as 61.67% and as high as 76.42%, on the unseen data."
      ],
      "metadata": {
        "id": "DWmp6RfCI8-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <b>Let's Apply Some Regularization Techniques.</b>"
      ],
      "metadata": {
        "id": "VKm3CGxpGLDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ⚙ Implementing Ridge (L-2) Regression."
      ],
      "metadata": {
        "id": "QWH54b4uJvEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the instance for ridge implementation\n",
        "ridge = Ridge()\n",
        "\n",
        "# Fit the Algorithm\n",
        "ridge.fit(x_train, y_train)\n",
        "\n",
        "# Show the scores\n",
        "print(f'Training Score : {round(ridge.score(x_train,y_train), 4)}')\n",
        "print(f'Testing Score : {round(ridge.score(x_test,y_test), 4)}')"
      ],
      "metadata": {
        "id": "KCSysF5BKHVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* The difference between the traning and testing score is also most 6.5%"
      ],
      "metadata": {
        "id": "xaGBOh65OBbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "check_pred(ridge)"
      ],
      "metadata": {
        "id": "K9aqe-TUibbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Actual VS Prediction - Ridge\n",
        "plt.figure(figsize = (10,5))\n",
        "plot_actual_vs_pred(model = ridge, model_name = 'Ridge')"
      ],
      "metadata": {
        "id": "ndqlwVlVeEUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* No major finding after application of ridge.\n",
        "\n",
        ">* The model is slightly better than base model and the improvement is under one percent."
      ],
      "metadata": {
        "id": "3N7Ao-u5eSxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "YhPgBgGDI-yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation metrics - Ridge Model\n",
        "update_p_metrics_df(model = ridge, model_name = 'Ridge' )"
      ],
      "metadata": {
        "id": "W_ZNaSPzNnwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Let's check the effects on coefficients."
      ],
      "metadata": {
        "id": "YP9wKIvzOBol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The regression coefficients are:\", ridge.coef_)\n",
        "\n",
        "print(\"The total regression coefficients are:\", len(ridge.coef_))"
      ],
      "metadata": {
        "id": "urywdFC9O4bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizing cofficients - Ridge\n",
        "plt.figure(figsize = (10,5))\n",
        "plot_coef(model = ridge, model_name = 'Ridge')"
      ],
      "metadata": {
        "id": "spD7e8X_Po78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* The changes are so minimal that it's almost seems same to the base version."
      ],
      "metadata": {
        "id": "kFWxxVwSPyci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for homoscedasticity - Ridge\n",
        "check_homoscedasticity(model = ridge, model_name = 'Ridge')"
      ],
      "metadata": {
        "id": "iITO_TfdQJox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We observed the same `V-Shape` distribution again."
      ],
      "metadata": {
        "id": "JT2iy2XdQemu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "_k_mbnbpdgN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ridge - Cross Validation r2-score and Performance Plot\n",
        "plt.figure(figsize = (10,5))\n",
        "plot_cv_perfomance(model = ridge, model_name = 'Ridge')"
      ],
      "metadata": {
        "id": "x3i7U8Q3OUMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We can except model accuracy as low as 61.62% and as high as 76.26%."
      ],
      "metadata": {
        "id": "Ny6uVyOfG5VH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ridge Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "ridge = Ridge()\n",
        "parameters = {'alpha': np.arange(-100,100,0.1)}\n",
        "ridge_regressor = GridSearchCV(ridge, parameters, scoring='r2', cv=tscv)\n",
        "\n",
        "# Fit the Algorithm\n",
        "ridge_regressor.fit(x_train, y_train)\n",
        "\n",
        "# Get the scores\n",
        "print(\"The best fit alpha value is found out to be :\" ,ridge_regressor.best_params_)"
      ],
      "metadata": {
        "id": "ujliY0ZDdgN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing ridge again with best alpha value which is found by the GridSearch CV\n",
        "ridge = Ridge( ridge_regressor.best_params_['alpha'])\n",
        "\n",
        "# Fit the Algorithm\n",
        "ridge.fit(x_train, y_train)\n",
        "\n",
        "print(f'Training Score : {ridge.score(x_train,y_train)}')\n",
        "print(f'Testing Score : {ridge.score(x_test,y_test)}')"
      ],
      "metadata": {
        "id": "3DQkvVTpMpKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We have seen the aprox. 5% reduction in the testing score."
      ],
      "metadata": {
        "id": "4B8rN_Joe-SD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "check_pred(ridge)"
      ],
      "metadata": {
        "id": "Nop0aEYPJrBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Actual VS Prediction - Ridge\n",
        "plt.figure(figsize = (10,5))\n",
        "plot_actual_vs_pred(model = ridge, model_name = 'Regularized_Ridge')"
      ],
      "metadata": {
        "id": "9UrUbGdtQUaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation metrics - Ridge Model\n",
        "update_p_metrics_df(model = ridge, model_name = 'Ridge' )"
      ],
      "metadata": {
        "id": "vXKYPE3dfeTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ⚙ Implementing Lasso (L-1) Regression."
      ],
      "metadata": {
        "id": "9SxlxyXwRIWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the instances for lasso Implementation\n",
        "lasso = Lasso()\n",
        "\n",
        "# Fit the Algorithm\n",
        "lasso.fit(x_train, y_train)\n",
        "\n",
        "# Let's check the scores\n",
        "print(f'Training Score : {lasso.score(x_train,y_train)}')\n",
        "print(f'Testing Score : {lasso.score(x_test,y_test)}')"
      ],
      "metadata": {
        "id": "utPYmG_tWf1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Testing Score improved even more after implementing Lasso with GridSearchCV parameter tunning.\n",
        "\n",
        ">* Lasso is definitly performing better than Ridge here.\n",
        "but also making model litle bit like under fit as testing score is more than training."
      ],
      "metadata": {
        "id": "ndCx1rnfSOIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "check_pred(lasso)"
      ],
      "metadata": {
        "id": "Q8Z2I1MskKwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Actual VS Prediction - Lasso\n",
        "plt.figure(figsize = (10,5))\n",
        "plot_actual_vs_pred(model = lasso, model_name = 'Lasso')"
      ],
      "metadata": {
        "id": "3fEXRf0Hgqah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Performing better than linear and ridge model.\n",
        "\n",
        ">* Lasso is performing much better in cross-validation than linear and ridge model."
      ],
      "metadata": {
        "id": "2-iYqPM7gyja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "kNxeqPRdQ_an"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation metrics - Lasso Model\n",
        "update_p_metrics_df(model = lasso, model_name = 'Lasso' )"
      ],
      "metadata": {
        "id": "2Lfl4Wp2L6tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Metric chart has been improved aftere Lasso implementation.\n",
        "R2-score and Adjusted-R2 both increased around 7 and 10 percent respectively. Also MSE and MAE are decreased."
      ],
      "metadata": {
        "id": "P4r4YvUJMJXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The regression coefficients are:\", lasso.coef_)\n",
        "\n",
        "print(\"The total regression coefficients are:\", len(lasso.coef_))"
      ],
      "metadata": {
        "id": "PYqNxJnyRQrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizing cofficients - Lasso\n",
        "plt.figure(figsize = (10,5))\n",
        "plot_coef(model = lasso, model_name = 'Lasso')"
      ],
      "metadata": {
        "id": "azzEiFl1P2YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* All the variable's coeficients become zero except OHL variable."
      ],
      "metadata": {
        "id": "Dm0KKjVOQG00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for homoscedasticity - Lasso\n",
        "check_homoscedasticity(model = lasso, model_name = 'Lasso')"
      ],
      "metadata": {
        "id": "w7vgo8RCQOBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We have observed the similar `V-Shape` again."
      ],
      "metadata": {
        "id": "xdTBjQeXRdxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "8aWQ5enMXChh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Lasso - Cross Validation r2-score and Performance Plot\n",
        "plt.figure(figsize = (10,5))\n",
        "plot_cv_perfomance(model = lasso, model_name = 'Lasso')"
      ],
      "metadata": {
        "id": "VdAsVA-tMnSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We can except the model accuracy as low as 68.24% and as high as 78.52%."
      ],
      "metadata": {
        "id": "cHInvVPJHJSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lasso Implementation with GridSearch CV hyperparameter optimization technique\n",
        "parameters = {'alpha': np.arange(-100,100,0.1)}\n",
        "lasso_regressor = GridSearchCV(lasso, parameters, scoring='r2', cv=tscv)\n",
        "\n",
        "# Fit the Algorithm\n",
        "lasso_regressor.fit(x_train, y_train)\n",
        "\n",
        "# Get the scores\n",
        "print(\"The best fit alpha value is found out to be :\" ,lasso_regressor.best_params_)"
      ],
      "metadata": {
        "id": "pPQawMrIXJoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing lasso again with best alpha value which is found by the GridSearch CV\n",
        "lasso = Lasso(lasso_regressor.best_params_['alpha'])\n",
        "\n",
        "# Fit the Algorithm\n",
        "lasso.fit(x_train, y_train)\n",
        "\n",
        "print(f'Training Score : {lasso.score(x_train,y_train)}')\n",
        "print(f'Testing Score : {lasso.score(x_test,y_test)}')"
      ],
      "metadata": {
        "id": "UByXYHRiXVUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "check_pred(lasso)"
      ],
      "metadata": {
        "id": "yQYHrFDcUQF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Actual VS Prediction - Ridge\n",
        "plt.figure(figsize = (10,5))\n",
        "plot_actual_vs_pred(model = lasso, model_name = 'Regularized_Lasso')"
      ],
      "metadata": {
        "id": "8NDLkdRsUPQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ⚙ Implementing ElasticNet Regression"
      ],
      "metadata": {
        "id": "U0Q_5n2NnHkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ElasticNet Implementation\n",
        "elasticnet = ElasticNet()\n",
        "\n",
        "# Fit the Algorithm\n",
        "elasticnet.fit(x_train, y_train)\n",
        "\n",
        "print(f'Training Score : {elasticnet.score(x_train,y_train)}')\n",
        "print(f'Testing Score : {elasticnet.score(x_test,y_test)}')"
      ],
      "metadata": {
        "id": "3v4FiVsunR4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We have seen the improvement in the scores but less than the lasso model\n",
        "\n",
        ">* The testing score is low when compaired to previous models.\n",
        "\n",
        ">* The overfitting is not present as the scores of train and test has less difference."
      ],
      "metadata": {
        "id": "d9Zpd0fGYujT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "check_pred(elasticnet)"
      ],
      "metadata": {
        "id": "AM6hID3mkn99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Actual VS Prediction - ElasticNet\n",
        "plt.figure(figsize = (10,5))\n",
        "plot_actual_vs_pred(model = elasticnet, model_name = 'ElasticNet')"
      ],
      "metadata": {
        "id": "NAkQOJsChEdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "UNcYWWAaVF19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation metrics - ElasticNet Model\n",
        "update_p_metrics_df(model = elasticnet, model_name = 'ElasticNet')"
      ],
      "metadata": {
        "id": "e-tERV2VqRkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* When compaired to lasso the ElasticNet has underperformed.\n",
        "\n",
        ">* The ElasticNet is slightly better model than the basic linear regression model."
      ],
      "metadata": {
        "id": "aqUVPec1qcg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The regression coefficients are:\", elasticnet.coef_)\n",
        "\n",
        "print(\"The total regression coefficients are:\", len(elasticnet.coef_))"
      ],
      "metadata": {
        "id": "MoODEcRrVLrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizing cofficients - ElasticNet\n",
        "plt.figure(figsize = (10,5))\n",
        "plot_coef(model = elasticnet, model_name = 'ElasticNet')"
      ],
      "metadata": {
        "id": "VERNkYylr7Ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for homoscedasticity - ElasticNet\n",
        "check_homoscedasticity(model = elasticnet, model_name = 'ElasticNet')"
      ],
      "metadata": {
        "id": "xoNco4eTsRmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We are not seeing any significant difference in the homoscedasticity, when compaired to the privious models.\n",
        "\n",
        ">* Along the course of all model we have seen the `V-Shaped` residuals plot throughtout, therefore let's apply `polyfit` so that we can conclude whether it improves the model performance or not."
      ],
      "metadata": {
        "id": "kjFubAWSsT2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "mtx-v5ZEoz9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  ElasticNet - Cross Validation r2-score and Performance Plot\n",
        "plt.figure(figsize = (10,5))\n",
        "plot_cv_perfomance(model = elasticnet, model_name = 'ElasticNet')"
      ],
      "metadata": {
        "id": "HAFn3FDUrDBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We can except accuracy of model as low as 62.21% and as high as 76.32%."
      ],
      "metadata": {
        "id": "Lw_iVaNYHbkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ElasticNet Implementation with GridSearch CV hyperparameter optimization technique\n",
        "parameters = {'alpha': np.arange(-10,10,0.01)}\n",
        "elasticnet_regressor = GridSearchCV(elasticnet, parameters, scoring='r2', cv=tscv)\n",
        "\n",
        "np.seterr(all='warn')\n",
        "\n",
        "# Fit the Algorithm\n",
        "elasticnet_regressor.fit(x_train, y_train)\n",
        "\n",
        "# Let's print the best fit alpha value\n",
        "print(\"The best fit alpha value is found out to be :\" ,elasticnet_regressor.best_params_)"
      ],
      "metadata": {
        "id": "G2iOXDJto3nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ElasticNet Implementation again with best alpha value which is found by the GridSearch CV\n",
        "elasticnet = ElasticNet(0.08999999999978492)\n",
        "\n",
        "# Fit the Algorithm\n",
        "elasticnet.fit(x_train, y_train)\n",
        "\n",
        "print(f'Training Score : {elasticnet.score(x_train,y_train)}')\n",
        "print(f'Testing Score : {elasticnet.score(x_test,y_test)}')"
      ],
      "metadata": {
        "id": "ksisBhj6pO4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "check_pred(elasticnet)"
      ],
      "metadata": {
        "id": "fopqE8pojerb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Actual VS Prediction - Ridge\n",
        "plt.figure(figsize = (10,5))\n",
        "plot_actual_vs_pred(model = lasso, model_name = 'Regularized_ElasticNet')"
      ],
      "metadata": {
        "id": "NwX8kfNMjerc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ⚙ Implementing Polynomial Fit"
      ],
      "metadata": {
        "id": "3B-4DNYDunKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying polynomial fit\n",
        "poly_pipe = Pipeline(((\"poly\",PolynomialFeatures(degree=2)), (\"lr\", LinearRegression())))\n",
        "\n",
        "# Fit the algorithm\n",
        "poly_pipe.fit(x_train,y_train)\n",
        "\n",
        "# Get the scores\n",
        "print(f\"Training R2:\", round(poly_pipe.score(x_train,y_train), 4))\n",
        "print(f\"Testing R2:\", round(poly_pipe.score(x_test,y_test), 4))"
      ],
      "metadata": {
        "id": "9Z_Rrfdtuy_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Trainning score is greater than testing score.\n",
        "\n",
        ">* The difference between the training score and testing score is almost 6.3%"
      ],
      "metadata": {
        "id": "vIFYkg-uifSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "check_pred(poly_pipe)"
      ],
      "metadata": {
        "id": "ma3C3jedlvb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Actual VS Prediction - Polyfit\n",
        "plt.figure(figsize = (10,5))\n",
        "plot_actual_vs_pred(model = poly_pipe, model_name = 'PolyFit')"
      ],
      "metadata": {
        "id": "tnnILeMm3hwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics - Ployfit\n",
        "update_p_metrics_df(model = poly_pipe, model_name = 'PolyFit')"
      ],
      "metadata": {
        "id": "hLy5t05o2_VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We have seen the improvement in all the matrics.\n",
        ">* R2-score and adjustable-R2 has been increased in Polyfit model.\n",
        ">* MSE and MAE are very low in PolyFit comparison to other models."
      ],
      "metadata": {
        "id": "tRBBxxUA5wdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Validation r2-score and Performance Plot - PolyFit\n",
        "plt.figure(figsize = (10,5))\n",
        "plot_cv_perfomance(model = poly_pipe, model_name = 'PolyFit' , y_lim = None)"
      ],
      "metadata": {
        "id": "fdyhF3gy4a0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* It's showing the overfitting at every fold, hence we can conclude that the polyfit is poorly performed in cross-validation."
      ],
      "metadata": {
        "id": "Ci0nVrxn4l6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for homoscedasticity - PolyFit\n",
        "check_homoscedasticity(model = poly_pipe, model_name = 'PolyFit')"
      ],
      "metadata": {
        "id": "7o0OR6Mt5Fwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We won't see any significant pattern in the resiudual plot."
      ],
      "metadata": {
        "id": "tUKyp9rX5IOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "XnteVDt0dgOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We have used the GridSearchCV for hyperparameter tunning in `L2`, `L1` and `ElasticNet` model.\n",
        "\n",
        ">* The reason for opt-out this perticular techique is because we have a very small dataset"
      ],
      "metadata": {
        "id": "3fVaejL9dgOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7-ixhLgKdgOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance evaluation metrics chart\n",
        "p_metrics_df"
      ],
      "metadata": {
        "id": "Vk8pi6Dr68n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* From the above chart it is clear that the `lasso` performed well in all aspects of the measured benchmark."
      ],
      "metadata": {
        "id": "_wHW7yGqdgOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2\n",
        "\n",
        "### ♠ <b> Random Forest Regressor"
      ],
      "metadata": {
        "id": "iryzxm-rdgOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://github.com/ashish-mali/Regression-Yes-Bank-Stock-Closing-Price-Prediction/blob/main/Images/random_forest_regression.png?raw=true\" alt=\"RandomForestConcept\" width=\"800\" height=\"500\"></center>"
      ],
      "metadata": {
        "id": "BiTsSPENPov_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Random Forest Regression is a type of machine learning algorithm that is used for regression tasks. It is an ensemble learning method that combines multiple decision trees and makes predictions based on their collective output.\n",
        "\n",
        ">* In a random forest regression model, a set of decision trees is created by randomly selecting subsets of the features and data points from the training data. Each decision tree is trained on this subset of data and produces a prediction for the target variable.\n",
        "\n",
        ">* During prediction, each decision tree produces its own output, and the final prediction is generated by averaging the outputs of all the trees. This helps to reduce overfitting and improve the accuracy of the model.\n",
        "\n",
        ">* The random forest algorithm can handle high-dimensional data and can work well with both categorical and continuous features. It can also handle missing data and outliers effectively.\n",
        "\n",
        ">* Overall, the random forest regression model is a powerful and flexible algorithm that can be used for a variety of regression tasks, including predicting stock prices, housing prices, and medical diagnosis."
      ],
      "metadata": {
        "id": "LtUP6BrsQbi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RandomForestRegressor Implementation\n",
        "rf = RandomForestRegressor(random_state=1)\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf.fit(x_train, y_train)\n",
        "\n",
        "# Getting the scores\n",
        "print(f'Training Score :', round(rf.score(x_train,y_train), 4))\n",
        "print(f'Testing Score :', round(rf.score(x_test,y_test), 4))"
      ],
      "metadata": {
        "id": "vj5NIJoZ-070"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* This model perform very well on both traning and test sets, also the difference among these are less."
      ],
      "metadata": {
        "id": "SduGGiyJ_pCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "check_pred(rf)"
      ],
      "metadata": {
        "id": "3ddCvIAXmUyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Actual VS Prediction - RandomForestRegressor\n",
        "plt.figure(figsize = (10,5))\n",
        "plot_actual_vs_pred(model = rf, model_name = 'RandomForest')"
      ],
      "metadata": {
        "id": "iC3d6-jvAir3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "I4UMh3rKdgOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "update_p_metrics_df(model = rf, model_name = 'Randomforest')"
      ],
      "metadata": {
        "id": "6QPFOyEadgOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Over all the benchmark matrics the performance is good."
      ],
      "metadata": {
        "id": "XiI4O8OxAJXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print feature importances\n",
        "importances = rf.feature_importances_\n",
        "for feature, importance in zip(X.columns, importances):\n",
        "    print(feature, ':', importance)"
      ],
      "metadata": {
        "id": "7Th7kHTO9nP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Regressor Feature Importance\n",
        "features = selected_df.drop('Close', axis = 1).columns\n",
        "skplt.estimators.plot_feature_importances(rf, feature_names=features,\n",
        "                                         title=\"Random Forest Regressor Feature Importance\",\n",
        "                                         x_tick_rotation=90,)"
      ],
      "metadata": {
        "id": "LR_v2vGTBs1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for homoscedasticity - RandomForest\n",
        "check_homoscedasticity(model = rf, model_name = 'RandomForest')"
      ],
      "metadata": {
        "id": "a0HP_1_mEH8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* There is no significant pattern observed.\n",
        "\n",
        ">* At some points the residuals are increasing with predicted values.\n",
        "\n",
        ">* At some instances there is high error for large predicated values."
      ],
      "metadata": {
        "id": "ONAFAHbZEQz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "HpxJH1vIdgOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  RandomForest - Cross Validation r2-score and Performance Plot\n",
        "plt.figure(figsize = (10,5))\n",
        "plot_cv_perfomance(model = rf, model_name = 'RandomForest')"
      ],
      "metadata": {
        "id": "-0IDC5DvA7pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We can expect the model accuracy as low as 90.71% and as high as 98.57%"
      ],
      "metadata": {
        "id": "u0HVpKEzBGvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RandomForestRegressor Implementation with GridSearchCV hyperparameter optimization technique\n",
        "parameters = {'n_estimators': [100,105,110,115,120,130,150],\n",
        "              'max_depth': [10,15,18,20,22,25,30,40,50],}\n",
        "\n",
        "start = datetime.now()\n",
        "grid_rf = GridSearchCV(RandomForestRegressor(), parameters, scoring='r2', cv=tscv)\n",
        "\n",
        "# Fit the Algorithm\n",
        "grid_rf.fit(x_train, y_train)\n",
        "\n",
        "# Get details\n",
        "print(f\"The best score : {grid_rf.best_score_} \\n\")\n",
        "print(f\"The best fit parameters are found out to be : {grid_rf.best_params_} \\n\")\n",
        "end = datetime.now()\n",
        "\n",
        "# Print the info\n",
        "execution_time = round((end - start).seconds/60,2)\n",
        "print(f'The computational time for finding the best parameters for random forest regressor model: {execution_time} minutes\\n')"
      ],
      "metadata": {
        "id": "KY5MBJEodgOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing Optimal Randomforest Model with best parameters\n",
        "optimal_rf = grid_rf.best_estimator_\n",
        "\n",
        "# Fit the Algorithm\n",
        "optimal_rf.fit(x_train, y_train)\n",
        "\n",
        "# Get the scores\n",
        "print(f'Training Score : {optimal_rf.score(x_train,y_train)}')\n",
        "print(f'Testing Score : {optimal_rf.score(x_test,y_test)}')"
      ],
      "metadata": {
        "id": "KPUgYo6LIw_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "check_pred(optimal_rf)"
      ],
      "metadata": {
        "id": "EC5SldXaofjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Actual VS Prediction - RandomForestRegressor\n",
        "plt.figure(figsize = (10,5))\n",
        "plot_actual_vs_pred(model = optimal_rf, model_name = 'Optimal_RandomForest')"
      ],
      "metadata": {
        "id": "dzANg_OYbYov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "z202NT4xdgOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We have used GridSearchCV for Random forest regressor hyperparameter tunning because here we have small data size and also tunning two parameters only."
      ],
      "metadata": {
        "id": "-mZufkW8dgOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "c5J-5pcrdgOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metric Score chart\n",
        "update_p_metrics_df(model = optimal_rf, model_name = 'Optimal_RandmForest')"
      ],
      "metadata": {
        "id": "gBalLfMOIWhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">We have seen improvement in all metric scores after random forest parameter tunning.\n",
        "\n",
        ">>* Little impromement is observed in the MAE AND MAPE.\n",
        "\n",
        ">>* R2 and adjusted_r2 both improve in little margins."
      ],
      "metadata": {
        "id": "QNOVJaphdgOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "XYCdNyj4dgOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <b><u>A brief description of each metric</u></b>\n",
        "\n",
        ">* <b>Mean Absolute Error (MAE):</b> MAE measures the average magnitude of the errors in a set of predictions. MAE is easy to understand and interpret, and it can be used to compare different models. A lower MAE indicates that the model is better at making predictions.\n",
        "\n",
        ">* <b>Mean Squared Error (MSE):</b> MSE measures the average of the squared differences between the predicted and actual values. MSE is a popular metric in regression problems and is useful when large errors are particularly important. However, it is sensitive to outliers and can be difficult to interpret because it is in squared units.\n",
        "\n",
        ">* <b>Root Mean Squared Error (RMSE):</b> RMSE is the square root of the MSE and is used to measure the magnitude of the error in the same units as the target variable. RMSE is a popular metric for regression problems because it is more interpretable than MSE.\n",
        "\n",
        ">* <b>Mean Absolute Percentage Error (MAPE):</b> MAPE measures the average percentage difference between the predicted and actual values. MAPE is particularly useful when the target variable has a wide range of values. However, it is sensitive to small values and can produce infinite values.\n",
        "\n",
        ">* <b>R-squared (R2) Score:</b> R2 measures the proportion of the variance in the target variable that is explained by the model. R2 is a popular metric for regression problems because it is easy to interpret and compare between models. A higher R2 indicates that the model is better at explaining the variance in the target variable.\n",
        "\n",
        ">* <b>Adjusted R-squared:</b> Adjusted R2 is a modified version of R2 that takes into account the number of predictors in the model. Adjusted R2 is useful for comparing models with different numbers of predictors. A higher adjusted R2 indicates that the model is better at explaining the variance in the target variable while taking into account the number of predictors.\n",
        "\n",
        "##### <b><u>Let's discuss the business context</u></b>.\n",
        "\n",
        "When evaluating the performance of machine learning models on stock market data, the choice of evaluation metrics depends on the specific business context and objectives. However, the following evaluation metrics can have a positive impact on businesses:\n",
        "\n",
        ">* <b>Mean Absolute Error (MAE):</b> MAE measures the average magnitude of the errors in a set of predictions. MAE is a useful metric for evaluating the accuracy of a model's predictions and is commonly used in finance to evaluate portfolio performance.\n",
        "\n",
        ">* <b>Root Mean Squared Error (RMSE):</b> RMSE is the square root of the MSE and is used to measure the magnitude of the error in the same units as the target variable. RMSE is a popular metric for regression problems and can provide insights into the volatility of a stock's returns.\n",
        "\n",
        ">* <b>R-squared (R2) Score:</b> R2 measures the proportion of the variance in the target variable that is explained by the model. R2 is a useful metric for evaluating the explanatory power of a model and can provide insights into the factors that drive a stock's returns.\n",
        "\n",
        ">* <b>Adjusted R-squared:</b> Adjusted R2 is a modified version of R2 that takes into account the number of predictors in the model. Adjusted R2 is useful for comparing models with different numbers of predictors and can provide insights into the relative importance of different factors in driving a stock's returns."
      ],
      "metadata": {
        "id": "yp7Brb8-dgOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3\n",
        "\n",
        "### ♠ <b> Extreme Gradient Boosting Regressor (XG-Boost)"
      ],
      "metadata": {
        "id": "waoaUkqCdgOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://github.com/ashish-mali/Regression-Yes-Bank-Stock-Closing-Price-Prediction/blob/main/Images/XGBoost_Regression.png?raw=true\" alt=\"XGBoostRegressionConcept\" width=\"1000\" height=\"500\"></center>"
      ],
      "metadata": {
        "id": "m7-CmkgtTQB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Extreme Gradient Boosting Regressor, or XGBoost Regressor, is a popular machine learning algorithm used for regression tasks. It is an extension of the Gradient Boosting Regressor algorithm that enhances its performance by using a more efficient implementation and advanced regularization techniques.\n",
        "\n",
        ">* The XGBoost Regressor works by creating an ensemble of decision trees, where each tree is trained to correct the errors of the previous tree.\n",
        "\n",
        ">* In this algorithm, decision trees are created in sequential form. Weights play an important role in XGBoost. Weights are assigned to all the independent variables which are then fed into the decision tree which predicts results. The weight of variables predicted wrong by the tree is increased and these variables are then fed to the second decision tree. These individual classifiers/predictors then ensemble to give a strong and more precise model."
      ],
      "metadata": {
        "id": "hY-665uvUQiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 XGBregressor Implementation\n",
        "xgbr = XGBRegressor(verbosity = 0)\n",
        "\n",
        "# Fit the Algorithm\n",
        "xgbr.fit(x_train, y_train)\n",
        "\n",
        "# Get the scores\n",
        "print(f'Training Score : {(xgbr.score(x_train,y_train))}')\n",
        "print(f'Testing Score : {(xgbr.score(x_test,y_test))}')"
      ],
      "metadata": {
        "id": "l06qk99BdgOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Traning and testing scores are very close to each other"
      ],
      "metadata": {
        "id": "TQFyA2nF-zzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "check_pred(xgbr)"
      ],
      "metadata": {
        "id": "DMmur_R5W0SU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Actual VS Prediction - XGBoost\n",
        "plt.figure(figsize=(10,5))\n",
        "plot_actual_vs_pred(model = xgbr, model_name = 'XGBoost')"
      ],
      "metadata": {
        "id": "SSji9rqyazpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We have seen the greater accuracy with this model compaired to others."
      ],
      "metadata": {
        "id": "FOYrbRStaIJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "hXduOjoddgOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "update_p_metrics_df(model = xgbr, model_name = 'XGBoost')"
      ],
      "metadata": {
        "id": "wt6q5xLodgOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* There is a slight improvement in all the matrics when we use the XGB."
      ],
      "metadata": {
        "id": "Jdps4ov6ao3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print feature importances\n",
        "importances = xgbr.feature_importances_\n",
        "for feature, importance in zip(X.columns, importances):\n",
        "    print(feature, ':', importance)"
      ],
      "metadata": {
        "id": "lArf1n3l_QUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost Regressor Feature Importance\n",
        "features = selected_df.drop('Close', axis = 1).columns\n",
        "skplt.estimators.plot_feature_importances(xgbr, feature_names=features,\n",
        "                                         title=\"XGBoost Regressor Feature Importance\",\n",
        "                                         x_tick_rotation=90,)"
      ],
      "metadata": {
        "id": "WE_e_gIVb1Ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* The `OHL` feature is the most important featue when it comes to the model predication. because it's highly correlated with the dependent variable."
      ],
      "metadata": {
        "id": "BBTCn96Jb96o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for homoscedasticity - XGBoost\n",
        "check_homoscedasticity(model = xgbr, model_name = 'XGBoost')"
      ],
      "metadata": {
        "id": "LTiX7SUMcYQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We observed no specific pattern here."
      ],
      "metadata": {
        "id": "wamRErJccaVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "Jk3Ac0G4dgOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  XGBoost - Cross Validation r2-score and Performance Plot\n",
        "plt.figure(figsize=(10,5))\n",
        "plot_cv_perfomance(model = xgbr, model_name = 'XGBoost')"
      ],
      "metadata": {
        "id": "-jwMaVSWbPBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We can expect the model performance as low as 90% and as high as 98.71%"
      ],
      "metadata": {
        "id": "YSOztqOObXE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# RandomForestRegressor Implementation with GridSearchCV hyperparameter optimization technique\n",
        "parameters = {'n_estimators': [100, 200,300,500],\n",
        "              'max_depth': [10, 18, 20,22],\n",
        "              'learning_rate' : [0.1, 0.2,1],\n",
        "              'colsample_bytree': [0.7, 0.8,0.9],\n",
        "              'reg_alpha': [1.1, 1.2, 1.3],\n",
        "              'reg_lambda': [1.1, 1.2, 1.3],\n",
        "              'subsample': [0.7, 0.8, 0.9]}\n",
        "start = datetime.now()\n",
        "optimal_xgbr = RandomizedSearchCV(xgbr, parameters, scoring='r2', cv=tscv, n_iter = 300, random_state = 0)\n",
        "\n",
        "# Fit the Algorithm\n",
        "optimal_xgbr.fit(x_train, y_train)\n",
        "\n",
        "# Get the score\n",
        "print(f\"The best score : {optimal_xgbr.best_score_} \\n\")\n",
        "print(f\"The best parameters : {optimal_xgbr.best_params_} \\n\")\n",
        "end = datetime.now()\n",
        "\n",
        "# Time calculation\n",
        "execution_time = round((end - start).seconds/60,2)\n",
        "print(f'The computational time for finding the best parameters for XGBoost regressor model: {execution_time} minutes\\n')"
      ],
      "metadata": {
        "id": "mbSx7DAodgOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing Optimal XGBoost Model with best parameters\n",
        "optimal_xgboost = optimal_xgbr.best_estimator_\n",
        "\n",
        "# Fit the Algorithm\n",
        "optimal_xgboost.fit(x_train, y_train)\n",
        "\n",
        "# Get the scores\n",
        "print(f'Training Score : {optimal_xgboost.score(x_train,y_train)}')\n",
        "print(f'Testing Score : {optimal_xgboost.score(x_test,y_test)}')"
      ],
      "metadata": {
        "id": "oaBe-6WleAI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "check_pred(optimal_xgboost)"
      ],
      "metadata": {
        "id": "GQx0b_nmeUgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Actual VS Prediction - XGBoost\n",
        "plt.figure(figsize=(10,5))\n",
        "plot_actual_vs_pred(model = optimal_xgboost, model_name = 'Optimal_XGBoost')"
      ],
      "metadata": {
        "id": "YVcnQ_ltdFSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "uyWNkW7TdgOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We have used RandomizedSearchCV for XGBRegressor hyperparameter tunning because here we are tunning many parameters at once and GridSearchCv will take too much time."
      ],
      "metadata": {
        "id": "VcEYhMXIdgOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "jKLiNmeVdgOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metric Score Chart\n",
        "update_p_metrics_df(model = optimal_xgboost, model_name = 'Optimal_XGBoost')"
      ],
      "metadata": {
        "id": "bEAk6v6bey7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* MSE is increase a bit.\n",
        ">* There is slight decrease in the other matrics."
      ],
      "metadata": {
        "id": "VxM7tJ_GdgOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "wMGHIsezdgOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">When predicting the closing price of a stock market, the choice of evaluation metric depends on the specific business context and objectives. However, the <b>Root Mean Squared Error (RMSE) can be a useful metric for this task.</b>\n",
        "\n",
        ">RMSE measures the square root of the average of the squared differences between the predicted and actual values. RMSE is commonly used in regression problems, and it can be a useful metric for evaluating the accuracy of a model's predictions when the target variable has a wide range of values, such as in stock market data. RMSE can provide insights into the volatility of a stock's returns, which is important in predicting the closing price of a stock.\n",
        "\n",
        ">However, it is important to note that other evaluation metrics, such as Mean Absolute Error (MAE), R-squared (R2) score, and adjusted R2, can also be useful in evaluating the performance of a model when predicting the closing price of a stock market. The choice of metric will depend on the specific objectives of the business and the problem at hand."
      ],
      "metadata": {
        "id": "mVnrjsLddgOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "2_LhwsYrdgOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the performance of the different models based on the evaluation metrics.\n",
        "\n",
        ">* <b>MAE, MSE, RMSE:</b> A lower value for these metrics indicates better performance, so based on these metrics, the PolyFit model, Randomforest, Optimal_RandmForest, XGBoost, and Optimal_XGBoost models appear to perform better than the other models.\n",
        "\n",
        ">* <b>MAPE:</b> A lower value for this metric indicates better performance. Based on this metric, the Lasso model appears to perform the best.\n",
        "\n",
        ">* <b>R2_score:</b> A higher value for this metric indicates better performance. Based on this metric, the Randomforest, Optimal_RandmForest, XGBoost, and Optimal_XGBoost models appear to perform better than the other models.\n",
        "\n",
        ">* <b>adjusted_r2:</b> A higher value for this metric indicates better performance. Based on this metric, the Randomforest, Optimal_RandmForest, XGBoost, and Optimal_XGBoost models appear to perform better than the other models.\n",
        "\n",
        "So, based on a combination of these metrics, it appears that the Randomforest, Optimal_RandmForest, XGBoost, and Optimal_XGBoost models may be good candidates for selection. However, further analysis is required to determine the optimal model for the specific business problem at hand.\n",
        "\n",
        "Let's see the various cross-validation scores to get the better picture.\n",
        "\n",
        ">* Mean cross-validation score for <b>linear regression is 0.6836</b>\n",
        ">* Mean cross-validation score for <b>ridge regression is 0.6882</b>\n",
        ">* Mean cross-validation score for <b>lasso regression is 0.7107</b>\n",
        ">* Mean cross-validation score for <b>elasticnet regression is 0.6138</b>\n",
        ">* Mean cross-validation score for <b>polynomial fit is -4.006$e^{+20}$</b>\n",
        ">* Mean cross-validation score for <b>random forest regression is 0.9577</b>\n",
        ">* Mean cross-validation score for <b>XGB regression is 0.9534</b>\n",
        "\n",
        "It appears that the Random Forest and XGB regressors have the highest mean cross-validation scores of 0.9577 and 0.9534, respectively. Therefore, one of these models may be the best choice for the problem at hand.\n",
        "\n",
        "To conform the model selectio we have again compute the cv score but this time we use different scoring technique."
      ],
      "metadata": {
        "id": "gO9UqOpNdgOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create the function which computes cross-validation scores for different regression models.\n",
        "def get_regression_cv_scores(models, X, y, scoring='neg_mean_squared_error', cv=5):\n",
        "\n",
        "    # Doc. string\n",
        "    \"\"\"\n",
        "    Computes cross-validation scores for different regression models.\n",
        "\n",
        "    Parameters:\n",
        "    models (dict): A dictionary of regression model instances.\n",
        "    X (array-like): The input data.\n",
        "    y (array-like): The target values.\n",
        "    scoring (str or callable): The scoring metric to use. Default is 'neg_mean_squared_error'.\n",
        "    cv (int): The number of cross-validation folds. Default is 5.\n",
        "\n",
        "    Returns:\n",
        "    A dictionary containing the mean and standard deviation of the cross-validation scores for each model.\n",
        "    \"\"\"\n",
        "    # Create a dictionary which holds the model name as key and scores as values\n",
        "    cv_scores = {}\n",
        "\n",
        "\n",
        "    for name, estimator in models.items():\n",
        "        if not hasattr(estimator, 'fit') or not hasattr(estimator, 'predict'):\n",
        "            continue  # Skip transformers\n",
        "        scores = cross_val_score(estimator, X, y, scoring=scoring, cv=cv)\n",
        "        cv_scores[name] = {\n",
        "            'mean': round(scores.mean() * -1, 4) * 100,\n",
        "            'std': round(scores.std(), 4) * 100\n",
        "        }\n",
        "    return cv_scores"
      ],
      "metadata": {
        "id": "TIV-eWiljyqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create the list of all regression model\n",
        "models = {'LinearRegression': LinearRegression(),\n",
        "          'Ridge': Ridge(),\n",
        "          'Lasso': Lasso(),\n",
        "          'ElasticNet': ElasticNet(),\n",
        "          'PolyFit': PolynomialFeatures(),\n",
        "          'Randomforest': RandomForestRegressor(),\n",
        "          'XGBoost': XGBRegressor()}"
      ],
      "metadata": {
        "id": "DWMVFpbrkqEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function calling\n",
        "cv_scores = get_regression_cv_scores(models, x, y)\n",
        "\n",
        "# Print the output\n",
        "for model, scores_dict in cv_scores.items():\n",
        "    print(f\"Model: {model}\")\n",
        "    print(\"*\"*100)\n",
        "    for metric, score in scores_dict.items():\n",
        "        print(f\"{metric}: {score}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "ByDmOWsmk1nD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon looking to the cv scores of different linear regression model we conclude the following things:\n",
        "\n",
        ">* The mean score indicates the average performance of the model across different splits of the data. A higher mean score suggests better performance.\n",
        "\n",
        ">* The standard deviation (std) measures the variability of the scores across different splits of the data. A lower standard deviation suggests that the model's performance is consistent across different splits of the data.\n",
        "\n",
        "Therefore, for the given models, the Lasso regression model has the highest mean score of 278198.6, suggesting that it has the best performance among the models. However, the ElasticNet model has a much higher standard deviation of 368551.28, indicating that its performance varies significantly across different splits of the data. On the other hand, the Random Forest and XGBoost models have a much lower standard deviation, suggesting that their performance is more consistent across different splits of the data."
      ],
      "metadata": {
        "id": "jjgypvNovdP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon observing and analyzing all performance matric and various cross validation score, we reaming with only two ml models which can perform better in all the conditions. those two models are:\n",
        ">* <b>Random Forest</b>  \n",
        ">* <b>XGBRegressor</b>\n",
        "\n",
        "Before, the final selection we have to consider the following factors:\n",
        "\n",
        ">* Interpretability\n",
        ">>* Simplicity\n",
        ">>* Transparency\n",
        ">>* Feature importance\n",
        ">>* Human validation\n",
        ">* Computation time\n",
        ">* Business requirements\n",
        "\n",
        "If we see through the above context than the <b>Optimal Random Forest</b> is the only model that can fit."
      ],
      "metadata": {
        "id": "lgydbK3zxLcU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "GvqsSTj1dgOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://github.com/ashish-mali/Regression-Yes-Bank-Stock-Closing-Price-Prediction/blob/main/Images/shape_theory.jpg?raw=true\" alt=\"Steps In Hypothesis_Testing_Title\" width=\"auto\" height=\"auto\"></center>"
      ],
      "metadata": {
        "id": "lTfZ5VhFsP_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* SHAP (SHapley Additive exPlanations) is a popular approach for explaining the output of machine learning models. It is a model-agnostic method that can be used to explain any type of model, including linear models, tree-based models, deep learning models, and more.\n",
        "\n",
        ">* The main idea behind SHAP is to decompose the output of a model for a given input into the contributions of each input feature. These contributions are calculated using the concept of Shapley values, which is a game-theoretic approach for allocating payouts among players based on their marginal contributions to the outcome.\n",
        "\n",
        ">* In the context of machine learning, the SHAP values can be interpreted as the average change in the model output when a feature value is included compared to when it is excluded, taking into account all possible subsets of features. By calculating the SHAP values for each feature, we can determine the importance of each feature in the model's output.\n",
        "\n",
        ">* One of the key benefits of SHAP is that it provides both local and global explanations of the model. Local explanations refer to the contributions of each feature for a specific prediction, while global explanations refer to the overall importance of each feature across all predictions.\n",
        "\n",
        ">* Overall, SHAP is a powerful and flexible method for explaining the output of machine learning models. It can help data scientists and machine learning practitioners gain a deeper understanding of how their models are making predictions, and identify areas where the model can be improved."
      ],
      "metadata": {
        "id": "aHqzCB_vdgOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load JS visualization code to notebook\n",
        "shap.initjs()"
      ],
      "metadata": {
        "id": "u922nYyyFl2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get shap values\n",
        "# Explain the model's predictions using SHAP values\n",
        "feature_names = selected_df.drop('Close', axis = 1).columns\n",
        "x_shap = x_train\n",
        "explainer = shap.TreeExplainer(optimal_rf)\n",
        "shap_values = explainer.shap_values(x_shap)"
      ],
      "metadata": {
        "id": "3PCzjxKBHB5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Explaining single prediction using force plot</b>"
      ],
      "metadata": {
        "id": "iiKgjJZQlawm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forceplot for first observation\n",
        "shap.force_plot(explainer.expected_value, shap_values[0,:], x_shap[0,:], matplotlib = True, feature_names = feature_names)"
      ],
      "metadata": {
        "id": "ptvzLarehe_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* The Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue.\n",
        "\n",
        ">* 'OHL' feature pushing the base value lower with very high impact.\n",
        "\n",
        ">* On the other hand, other features are pushing the price up but they have very low impact on the prediction."
      ],
      "metadata": {
        "id": "L0GvtLt0iMNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>SHAP Summary Plot</b>"
      ],
      "metadata": {
        "id": "G-JvO0mFjKv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shap feature summary plot\n",
        "feature_names = selected_df.drop('Close', axis = 1).columns\n",
        "shap.summary_plot(shap_values, x_shap, feature_names = feature_names)"
      ],
      "metadata": {
        "id": "nqnh0F0nJcdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* High OHL value increases the predicted closing price.\n",
        ">* Low OHL increases the predicted closing price.\n",
        ">* After OHL the second most important feature is Year."
      ],
      "metadata": {
        "id": "lEe0ZZt9iwhG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>SHAP Feature Importance Plot</b>"
      ],
      "metadata": {
        "id": "uVe-dHZpjCIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature importance plot using shap\n",
        "shap.summary_plot(shap_values, x_shap, feature_names = feature_names, plot_type=\"bar\")"
      ],
      "metadata": {
        "id": "Pvlg_yZUN135"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***\n",
        "<hr>"
      ],
      "metadata": {
        "id": "P2fONXH8dgOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "EYWJUD1ddgOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* We used the `joblib` for saving the model due to it's capacity of handling large `NumPy`arrays.\n",
        "\n",
        ">* Also `joblib`, is part of the Scikit-learn library, and is designed specifically for efficiently storing and retrieving large numpy arrays. It can also be used to save and load Scikit-learn models, and has some additional features such as support for memory-mapped arrays."
      ],
      "metadata": {
        "id": "KW7gdbZVU20p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model ina variable.\n",
        "best_model = optimal_rf\n",
        "\n",
        "# Save the File\n",
        "# Save the best model in a joblib file\n",
        "joblib.dump(best_model, 'best_model.joblib')"
      ],
      "metadata": {
        "id": "1o7vijYAdgOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "Ip1y9ivgdgOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "# To load the model from the saved joblib file\n",
        "loaded_model = joblib.load('best_model.joblib')"
      ],
      "metadata": {
        "id": "9a5OuV7LdgOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the loaded model for making predictions\n",
        "# Make predictions on the test data using the loaded model\n",
        "y_pred = loaded_model.predict(x_test)\n",
        "\n",
        "y_pred"
      ],
      "metadata": {
        "id": "dfMQty7mR9bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(y_test)"
      ],
      "metadata": {
        "id": "JtchPViNTlsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the DataFrame of test and train Dataset\n",
        "train_Dataset = pd.DataFrame(x_train,y_train)\n",
        "test_Dataset = pd.DataFrame(y_test)\n",
        "test_Dataset.rename(columns= {'Close' :'Actual Closing Price'}, inplace =True)"
      ],
      "metadata": {
        "id": "acUgqBpGT20X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_Dataset['Predicted Closing Price']= y_pred\n",
        "test_Dataset.head()"
      ],
      "metadata": {
        "id": "bYEYkotGT7n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "9n59s7J0dgOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "\n",
        "# <center> <font face=\"Lato\" color='red'>✴ Conclusion\n",
        "\n",
        "<hr>"
      ],
      "metadata": {
        "id": "zE6yxCSPdgOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. The yes bank stock price dataset does not contain any null/missing value, also it's free form outliers.\n",
        "\n",
        "#### 2. While doing data visualization and cleaning we came to the following conclusion:\n",
        ">* Some of the features are right-skewed so we have performed the log transformation.\n",
        ">* The dependent variable having string linear correlation with all independent variables.\n",
        ">* There is high-multicollinearity present in the data, so we introduce some new features.\n",
        ">* There is sudden drop in the value of stock after 2018\n",
        ">*  We have seen the sudden increase in the price of stock in 2014 in  a window of 10 months.\n",
        ">* The VIFs values are extremely large, so we drop some features to reduce the VIFs scores.\n",
        ">* We used the StandardScaler to scale our features.\n",
        "\n",
        "#### 3. In the hypothesis testing we mostly find the status quo, and one important conclusion is that the impact of COVID-19 is less as compared to the scam in 2020.\n",
        "\n",
        "#### 4. We have implemented the following regression models, so that we can assure that we get the best fit model.\n",
        ">* Linear Regression\n",
        ">* Ridge Regression\n",
        ">* Lasso Regression\n",
        ">* Polynomial Fit\n",
        ">* Random Forest Regressor\n",
        ">* eXtreme Gradient Boosting (XGBoost) Regressor.\n",
        "\n",
        "#### 5. Upon implementing the given regression models, we came to the following important conclusions:\n",
        ">* The Lasso performed better when compared to linear, ridge, polyfit.\n",
        ">* Polynomial fit performed bad in cross-validation as it shows the sign of overfitting.\n",
        ">* The RandomForest and XGBoost are the final nominees for the model selection. as their overall performance in all matrices and in cross-validation are good compared to others.\n",
        ">* Finally we choose the RandomForest regressor, taking into consideration time-complexity and cross-validation score (mean CV = 95.77).\n",
        ">* After model selection we used SHAP score to explain the model by using various plots, and came to know that OHL feature dominates the other feature as its correlation with DV is high and the second most important feature is Year.\n",
        ">* At the end of the project we save our selected model in joblib and perform some sanity checks."
      ],
      "metadata": {
        "id": "27i-Y0fndgOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! We have successfully completed our Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "XkR4yQXfdgOy"
      }
    }
  ]
}